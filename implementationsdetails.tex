\chapter{Umsetzung}
\label{chap:implementierung}
In diesem Kapitel werden die Funktionalitäten der Pipeline zusammengetragen. Um die Anforderungen genauer zu beschreiben, wird ein Use-Case-Diagramm und ein Flussdiagramm dargestellt und beschrieben. Zudem werden konkrete funktionale und nichtfunktionale Anforderungen des Systems erfasst. Abschließend werden verwendete Technologien und Implementationsaspekte genauer beschrieben.
\section{Beschreibung des entwickelten Systems}
\label{sec:beschreibungsystem}
Es wird eine Lösung gesucht, die eine effiziente Verarbeitung von \emph{Bedarfsmeldungen} durchführen kann. Welche Aspekte in einer Bedarfsmeldung relevant sind, wurde bereits im Kapitel \ref{chap:erwartungshaltung} näher erläutert. Auf Basis welcher Methodiken und Ansätze die relevanten Informationen extrahiert werden können, wurde in Kapitel \ref{sec:literaturueberblick} dargestellt. Nun gilt es eine Lösung zu entwickeln, die diese Ansätze in einem System implementiert. Die Idee ist es, eine Pipeline zu entwickeln, die Möglichkeiten zum laden von \emph{Bedarfsmeldungen} bietet. Das System überführt die Informationen aus Jira in eine Struktur, die für die weitere Nutzung im Recommender System Kontext funktioniert. In den \emph{Bedarfsmeldungen} existieren semi strukturierte Volltexte. Diese gilt es mit den Erforschten Ansätzen durch Ablauf verschiedener Schritte innerhalb des System zu bearbeiten. Damit alle Ansätze und Methoden zur Extraktion von Informationen gut funktionieren und vergleichbar bleiben, wird eine Übersetzungsfunktion der \emph{Bedarfsmeldungen} benötigt. Auch wenn die \emph{Bedarfsmeldungen} in den meisten Fällen auf Deutsch sind, hilf es diese zu übersetzen, damit keine Unterschiede in der Ergebnisqualität resultiert, da einige Methoden und Ansätze auf Basis von Englischen Trainingssätzen trainiert wurden. Schließlich müssen alle aus Kapitel \ref{sec:literaturueberblick} untersuchten Ansätze implementiert und nutzbar sein. Sie sollen die Möglichkeit haben \emph{Bedarfsmeldungen} als Input zu erhalten und eine strukturierte \emph{Bedarfsmeldung} als Ausgabe zurückzugeben. Zur vereinfachten Entwicklung soll das System modular sein, damit Methoden und Ansätze nach belieben durchgetauscht und verwendet werden können.\\
\section{Use-Case}
\label{sec:usecase}
In diesem Kapitel werden die Interaktionen zwischen Benutzer und System beschrieben. Dazu wird ein Use-Case-Diagramm angefertigt, das eine grafische Übersicht über alle Anwendungsfälle bietet.
\begin{figure}[H]
	\centering  
	\includegraphics[width=\linewidth]{Abbildungen/use-case.png}
	\caption{Use-Case Diagramm zum System zur Strukturierung von Bedarfsmeldungen.}
	\label{fig:usecasediagrammwirklich}
\end{figure}\mbox{} \\
In der Abbildung \ref{fig:usecasediagrammwirklich} ist das Use-Case Diagramm zum System zur Strukturierung von \emph{Bedarfsmeldungen}. Es existiert ein Akteur, nämlich der Benutzer. Dieser hat die Möglichkeit unstrukturierte Bedarfsmeldungen zu strukturieren. Dazu kann das System einerseits eine \emph{Bedarfsmeldung} laden und im Falle der unstrukturierten Volltexfelder eine Reihe an Aufgaben durchführen. Um die Volltexte in Stichpunkte zu überführen, kann die \emph{Bedarfsmeldung} übersetzt werden. Zudem kann diese mit Preprocessing-Verfahren reduziert und angepasst werden. Im Rahmen der Überführung von Stichpunkten können die aus den Volltext erstellten \emph{n-Gramme} auf die \emph{n-Gramme}, die mindestens ein Schlüsselwort enthalten gekürzt werden. Die Schlüsselwörter werden mit der \emph{TF-IDF}-Methode ermittelt. Im Anwendungsfall der Überführung des Volltextes in Stichpunkte, können Wortartkombinationen, die nicht zusammengehören entfernt werden. Dadurch trennen sich neben der Filterung von Schlüsselwörter-\emph{n-Grammen} weitere \emph{n-Gramm}-Verbindungen. Schließlich können die \emph{n-Gramme} zu Stichpunkten überführt werden.
%Dieser hat die Möglichkeit eine oder mehrere \emph{Bedarfsmeldungen} zu laden. Zudem kann diese ins Englische übersetzt werden. Darüber hinaus kann die \emph{Bedarfsmeldung} durch \emph{Preprocessing} vorverarbeitet werden und irrelevante Wörter und Zeichen aus der \emph{Bedarfsmeldung} ausschließen. Der Benutzer kann die Methoden \emph{TF-IDF}, \emph{TextRank}, \emph{N-Gram}, \emph{POS-Tagging} und \emph{NER} mit der \emph{Bedarfsmeldung} anwenden. Wenn mehrere Methoden in der Pipeline sind, können die Resultate durch die Anwendung von \emph{Data-Fusion} zusammengeführt werden.
\section{Anforderungen}
Im Folgenden werden die funktionalen sowie nichtfunktionalen Anforderungen des Systems beschrieben. Diese Informationen wurden aus der Beschreibung des Systems aus Kapitel \ref{sec:beschreibungsystem} und dem Use-Case aus Kapitel \ref{sec:usecase} hergeleitet und bilden den Rahmen des Systems.
\subsection{Funktionale Anforderungen}
Die funktionalen Anforderungen beschreiben konkrete Funktionalitäten des Systems. Dazu werden zusammengehörende Anforderungen nummeriert und in detaillierte Unterpunkte aufgelistet und beschrieben.
\begin{enumerate}[label=1.\arabic*]
	\item Die \emph{Bedarfsmeldungen} sollen geladen werden können.
	\item Beim laden wird eine \emph{Bedarfsmeldung} ausgewählt.
	\item Die ausgewählte \emph{Bedarfsmeldung} muss in die vordefinierte Struktur überführt werden.
\end{enumerate}
\begin{enumerate}[label=2.\arabic*]
	\item Ausgewählte Felder der \emph{Bedarfsmeldung} sollen ins Englische übersetzt werden können.
	\item Ausgewählte Felder der \emph{Bedarfsmeldung} sollen zurück ins Deutsche übersetzt werden können.
\end{enumerate}
\begin{enumerate}[label=3.\arabic*]
	\item Das System soll Methoden des Preprocessing zur Bereinigung eines Volltextes anwenden können.
	\item Das Preprocessing soll ein Volltext als Eingabe erhalten.
	\item Das Preprocessing soll den bereinigten Volltext als Ausgabe zurückgeben.
\end{enumerate}
\todo{hier nochmal gucken}
\begin{enumerate}[label=4.\arabic*]
	\item Das System soll die Methode \emph{TF-IDF} als Modul anwenden können.
	\item Das Modul soll ein Volltext als Eingabe für die implementierte Methode erhalten.
	\item Als Rückgabe soll das Modul eine Liste mit Schlüsselwörtern basierend auf die Scores aus der \emph{TF-IDF} zurückgeben.
\end{enumerate}
\begin{enumerate}[label=5.\arabic*]
	\item Das System soll \emph{Bi-Gramme} aus einem Volltext erstellen können.
	\item Das Modul soll ein Volltext als Eingabe für die implementierte Methode erhalten.
	\item Als Rückgabe soll das Modul eine Liste mit \emph{Bi-Grammen} zurückgeben.
\end{enumerate}
\begin{enumerate}[label=6.\arabic*]
	\item Das System soll eine \emph{Bi-Gramm} Liste auf die \emph{Bi-Gramme} reduzieren, die mindestens ein Schlüsselwort enthalten.
\end{enumerate}
\begin{enumerate}[label=7.\arabic*]
	\item Das System soll die Methode \emph{POS-Tagging} als Modul anwenden können.
	\item Das Modul soll immer ein \emph{Bi-Gramm} erhalten und überprüfen ob die Wörter nebeneinander stehen dürfen.
	\item \emph{Bi-Gramm}-Kombinationen die nicht nebeneinander stehen sollen, werden entfernt.
\end{enumerate}
\begin{enumerate}[label=8.\arabic*]
	\item Das System soll die \emph{Bi-Gramme} in zusammengehörende Stichpunkte zusammenführen.
	\item Als Ergebnis wird eine Liste mit Stichpunkten zurückgegeben.
\end{enumerate}
\begin{enumerate}[label=9.\arabic*]
	\item Die in Stichpunkte umgebauten Volltexte werden der Bedarfsmeldung beigefügt.
	\item Das Ergebnis wird dem Benutzer ausgegeben.
\end{enumerate}
\subsection{Nichtfunktionale Anforderungen}
Hierbei handelt es sich um qualitätsbezogene Anforderungen. Diese umfassen nicht konkrete Funktionen des Systems, sondern stellen Rahmenbedingungen des Systems im Ganzen zusammen.
\begin{enumerate}
	\item Das System soll modular aufgebaut sein, um die einzelnen Schritte und Methoden austauschen zu können.
	\item Das System soll in der Lage sein, mehrere \emph{Bedarfsmeldungen} laden zu können.
	\item Das System muss in der Lage sein, die Bedarfsmeldungen in einer akzeptablen Zeitspanne zu verarbeiten.
	\item Das Ergebnis muss deterministisch sein.
\end{enumerate}
\section{Systemablauf}
Dieses Kapitel beschreibt den Ablauf der Pipeline. Dazu wird ein Flussdiagramm mit allen Komponenten des Systems beschrieben.
\begin{figure}[H]
	\centering  
	\includegraphics[width=\linewidth]{Abbildungen/flowchart.png}
	\caption{Flussdiagramm der Pipeline.}
	\label{fig:flowchart}
\end{figure}\mbox{} \\
Die Abbildung \ref{fig:flowchart} zeigt das Flussdiagramm der Python Pipeline. Im ersten Schritt \emph{Bedarfsmeldungen laden} im Ablauf der Pipeline wird eine \emph{Bedarfsmeldung} ausgewählt und in die festgelegte \emph{Bedarfsmeldungsstruktur} aus Kapitel \ref{sec:strukturierungbedarfsmeldung} umgebaut. Die Felder \emph{Einsatzbeginn} und \emph{Einsatzende} werden zu einem Feld \emph{Einsatz} zusammengefügt. Neben den Feldern \emph{Aufgaben} und \emph{Skills} bleiben alle weiteren Felder bis zum letzten Punkt \emph{Bedarfsmeldung zusammenfügen und ausgeben} unverändert. Die Felder \emph{Aufgaben} und \emph{Skills} werden in die parallele Struktur weitergeleitet, wo sie auf Stichpunkte reduziert werden. Der Prozess zur Reduktion durchläuft mehrere Schritte. Zu Beginn werden die Volltexte aus den Feldern im Punkt \emph{Übersetzung} ins Englische übersetzt und in Punkt \emph{Preprocessing} für die weitere Nutzung vorverarbeitet. Dieser Text wird Einerseits für die Ermittlung von relevanten Wörtern durch die \emph{TF-IDF}-Methode und zum anderen zur Erstellung von \emph{Bi-Grammen} verwendet. Beim Punkt \emph{filter Bi-Grams} werden alle \emph{Bi-Gramme} entfernt, bei denen kein relevantes Wort aus der TF-IDF-Methode enthalten sind. Somit erhält der Schritt \emph{POS-Tagging} eine reduzierte Liste mit \emph{Bi-Grammen}, bei dem mindestens eines der beiden \emph{Bi-Gramm}-Wörter ein relevantes Wort darstellt. Jedes Wort der \emph{Bi-Gramm}-Liste durchläuft eine weitere Filterung. Im Englischen existieren Wortarten, die untypischerweise nebeneinander stehen. Durch Entfernung dieser \emph{Bi-Gramme} durch diese POS-Tagging-Kombinationen erfolgt eine weitere Trennung der Wortketten der \emph{Bi-Gramm}-Liste. Wörter die nicht zusammengehören, verlieren dadurch die Verbindung zueinander. Im darauf folgenden Schritt \emph{Bi-Gramm zu Stichpunkten} werden die \emph{Bi-Gramme} jeweils zu einem String zusammengefügt, bei dem im darauf Folgendem \emph{Bi-Gramm} ebenfalls ein relevantes Wort enthalten sind. Dadurch formt sich eine Liste mit Stichpunkten bestehend aus eins bis x vielen Wörtern, die Bezug zueinander haben. Im Schritt \emph{Übersetzen} wird die Liste mit Stichpunkten zurück ins deutsche Übersetzt und im Schritt \emph{Bedarfsmeldung zusammenfügen und ausgeben} in die \emph{Bedarfsmeldung} aufgenommen. Dieser parallele Ablauf wird für die beiden Felder \emph{Aufgaben} und \emph{Skills} durchlaufen. Zum Schluss wird die fertige \emph{Bedarfsmeldung} ausgegeben.
\section{Details zur Implementierung der Pipeline}
Dieses Kapitel beschreibt den technischen Entwicklungsprozess zur Umsetzung der Anforderungen des Systems. Die Implementierung fokussiert sich auf die Umsetzungen von Technologien und Funktionsweisen verschiedener Anforderungen. Zudem wird die Struktur des Projektes aufgezeigt.
\subsection{Umsetzung}
Die Pipeline wurde in der Programmiersprache Python umgesetzt. Python hat sich zu einer der populärsten interpretierten Programmiersprachen entwickelt \cite{mckinney2012python}. Die Programmiersprache eignet sich insbesondere für die Erstellung kleiner Programme und Skripte, die zur Automatisierung von Aufgaben eingesetzt werden können \cite{mckinney2012python}. Python hat eine große und aktive Community für wissenschaftliche Berechnungen und Datenanalysen hervorgebracht und hat sich in den letzten Jahren zu einer der wichtigsten Sprachen für Data Science, maschinelles Lernen und allgemeine Softwareentwicklung in Wissenschaft und Industrie entwickelt \cite{mckinney2012python}. Python unterstützt Modularität, wodurch ein Teil der Anforderungen somit abgedeckt werden kann. Die Implementierungen der einzelnen Verfahren aus den Anforderungen werden nicht manuell, sondern auf Basis von bereits existierende Bibliotheken umgesetzt.
\subsection{Projektstruktur}
\label{sec:projektstruktur}
Das Projekt wird in einem git Repository gespeichert und versioniert. Die Projektstruktur ist ohne zusätzliche Konfigurationsdateien wie folgt aufgebaut:
\dirtree{%
	.1 .git/.
	.2 modules/.
	.3 nGram.py.
	.3 posTagging.py.
	.3 preprocessing.py.
	.3 readRequirements.py.
	.3 textRankingAlgorithm.py.
	.3 tfIdf.py.
	.3 transformRequirements.py.
	.3 translate.py.
	.2 requirements/.
	.3 jiraTickets.json.
	.3 preprocessedRequirements.json.
	.3 ....
	.2 app.py.
	.2 ....
}
Die einzelnen Module aus dem Flussdiagramm in Kapitel \ref{fig:flowchart} sind im Verzeichnis \url{modules/} in separaten \url{.py} Dateien gelagert. Die \emph{Bedarfsmeldungen} werden im \url{requirements/}-Verzeichnis gespeichert. Alle relevanten \emph{Bedarfsmeldungen} sind in der \url{jiraTickets.json}-Datei in einer Liste gespeichert. Die Datei \url{app.py} ist der Kern der Pipeline. Diese importiert alle Module und implementiert die Struktur der Pipeline. Das Projekt kann über den Befehl \lstinline{> py app.py} ausgeführt werden.
\subsection{Implementationsdetails}
Nachfolgend werden Implementationsdetails zu den einzelnen Modulen gegeben. Dabei werden verwendete Bibliotheken und Code-Details näher erläutert.
\paragraph{Dateiformat}\mbox{}\\
Die \emph{Bedarfsmeldungen} wurden über die Jira-Schnittstelle extrahiert und im Verzeichnis \url{requirements/jiraTickets.json} gespeichert. Zur Eingrenzung der Datenmenge wurde ein Filter angewendet, der nur die \emph{offenen} und \emph{eskalierten} \emph{Bedarfsmeldungen} zurückgibt. Diese sind die relevanten und noch aktuellen \emph{Bedarfsmeldungen}. Zudem wurden alle unrelevanten Felder mit einem weiteren Filter herausgenommen. Die Daten aus der Jira-API bestehen namentlich aus \emph{customfields} mit einer angehangenden ID. Der Softwareprototyp lädt in dem Modul \emph{readRequirements.py} die unstrukturierten Fields und formt diese in dem Modul \emph{transformRequirements.py} in \emph{Bedarfsmeldungs}-Objekte um.
\begin{center}
	\begin{tabularx}{1\textwidth} { 
			| >{\raggedright\arraybackslash}X 
			| >{\raggedright\arraybackslash}X
			| >{\raggedright\arraybackslash}X | }
		\hline
		Display-Felder & Jira-API-Felder & Objekt-Felder \\
		\hline
		\hline
		Überschrift & summary & header\\
		\hline
		Rolle & customfield\_15321 & role\\
		\hline
		Aufgaben & customfield\_10288 & tasks\\
		\hline
		Skills & customfield\_10296 & skills\\
		\hline
		Skill-Level & customfield\_15322 & skillLevel\\
		\hline
		Kunde & customfield\_10279 & customer\\
		\hline
		Einsatzort & customfield\_10297 & location\\
		\hline
		Beginn & customfield\_10293 & timePeriod\\
		\hline
		Ende & customfield\_10294 & timePeriod\\
		\hline
		Tagessatz & customfield\_10298 & dailyRate\\
		\hline
	\end{tabularx}\\
	\captionof{table}{Übersicht der Datenfelder}
	\label{tab:jiradaten}
\end{center}
In der Tabelle \ref{tab:jiradaten} ist in der ersten Spalte eine Übersicht der Datenfelder, wie diese in der Abbildung \ref{fig:jiraafter} mit dem Mockup einer standardisierten \emph{Bedarfsmeldung} auftauchen. In der zweiten Spalte sind die dazugehörigen Feldernamen, die in der \url{jiraTickets.json} enthalten sind. Die dritte Spalte spiegelt die jeweiligen Namen innherhalb des  \emph{Bedarfsmeldungs}-Objekts im Prototypen wieder. Das  \emph{Bedarfsmeldungs}-Objekt dient der strukturierten Handhabung der \emph{Bedarfsmeldungs}-Daten innerhalb des Systems.

%Um einen Freitext aus einer einzelnen \emph{Bedarfsmeldung} für die Pipeline zu laden werden Daten mit der Endung \url{.txt} verwendet.
%\begin{lstlisting}[caption={Implementation der Methode read() des Moduls \emph{readRequirements.py}}, label=lst:read]
%	def read(filename):
%		path = os.path.join(PATH, filename)
%		file = open(path, "r", encoding="utf-8")
%		content = file.read()
%		file.close()
%		return content
%\end{lstlisting}
%Im Listing \ref{lst:read} ist die Implementierung der Methode zum laden eines Freitextes dargestellt. Durch ein mitgelieferten Parameter \emph{filename} wird der Name der Datei in Zeile 2 an den Pfad angehängt. Mit der Methode \lstinline{open()}
%aus Zeile 3 kann die Datei geladen werden. Die Methode erhält die Parameter Pfad, \emph{'r'} (read) und das encoding \emph{'utf-8'}. Das encoding ist dabei Entscheidend, damit die unstrukturierten \emph{Bedarfsmeldungen} laden können. Bei der Pflege in Jira wird wenig Wert auf eine einheitliche Struktur. Somit können Zeichen enthalten sein, die beim öffnen nicht erkannt werden und eine Fehlermeldung wird zurückgegeben. Zur Vermeidung dieses Fehlers wird das encoding festgelegt. Nach dem laden durch die Methode \lstinline{read()} wird der Inhalt der \url{.txt} Datei in der Variable \emph{content} gespeichert und zurückgegeben.
\paragraph{Übersetzung}\mbox{}\\
Das Modul \emph{translate.py} ist dazu da, um die \emph{Bedarfsmeldungen} zu übersetzen. Für die Übersetzung wurde die Python Bibliothek \emph{deep-translator} verwendet. Diese bietet Implementationen unterschiedlicher Übersetzungs-APIs von diversen Anbietern. Der Vorteil ist dabei die vereinfachte Möglichkeit Anbieter bei Bedarf zu wechseln.
%\begin{lstlisting}[caption={Implementation des Moduls \emph{translate.py}}, label=lst:translate]
%	from deep_translator import GoogleTranslator
%	
%	def translate(text):
%		translated = GoogleTranslator(source='auto', target='en').translate(text)
%		return translated
%\end{lstlisting}
%Das Listing \ref{lst:translate} zeigt die Implementation des Moduls. 
Es wurde sich für den Google Translator entschieden, da hierfür kein API-Key benötigt wird. Die Methode erhält einen Text als Parameter. Der Google Translator erhält die Parameter \emph{source} und \emph{target}, beidem \emph{source} angibt in welcher Sprache der Eingabetext ist. Durch Angabe von \emph{'auto'} wird die Sprache ermittelt. Der Grund dafür ist, dass grundsätzlich anderssprachige \emph{Bedarfsmeldung} enthalten sein können. Der Parameter \emph{target} ist die Zielsprache in welche der Input übersetzt werden soll. Die Zielsprache ist hier Englisch (\emph{'en'}).
\paragraph{Preprocessing}\mbox{}\\
Vor der weiteren Nutzung der Daten innerhalb einer \emph{Bedarfsmeldung}, ist es erforderlich diese von irrelevanten Wörtern, Zeichen und Formatierungen zu befreien.% Zur Entfernung von Satzzeichen wurde die Bibliothek \emph{string} verwendet.
%\begin{lstlisting}[caption={Implementation der Methode removePunctuation() des Moduls \emph{preprocessing.py}}, label=lst:punctuation]
%	import string
%	def removePunctuation(text):
%		content=""
%		for i in text: 
%			if i not in string.punctuation:
%				content+=i    
%		return content
%\end{lstlisting}
%Im Listing \ref{lst:punctuation} ist die Implementierung dargestellt. Die \emph{Bedarfsmeldung} wird in Zeile 2 als Parameter übergeben. In Zeile 4 erfolgt ein Durchlauf jedes Zeichens innerhalb der \emph{Bedarfsmeldung}. Falls innerhalb der Schleife das Aktuelle Zeichen kein Satzzeichen enthält wird dieses in die Variable \emph{content} zwischengespeichert. Nach Abschluss der Schleife wird die Variable zurückgegeben. 
Zur Eliminierung wiederaufgetretener Wörter, die keine Relevanz für den Informationsgehalt aufweisen, wurde die Bibliothek \emph{nltk} verwendet. Diese beinhaltet eine Liste an sogenannten \emph{stopwords}.
%\begin{lstlisting}[caption={Implementation der Methode removeStopwords() des Moduls \emph{preprocessing.py}}, label=lst:stopwords]
%	from nltk.corpus import stopwords
%	def removeStopwords(text):
%		words=[word for word in text.split(" ") if word not in set(stopwords.words('english'))]
%		return " ".join(str(word) for word in words)
%\end{lstlisting}
%Im Listing \ref{lst:stopwords} ist die Implementierung zur Entfernung von \emph{stopwords} dargestellt. In Zeile 3 
Es werden alle mit einem Leerzeichen getrennten Wörter aus der übergebenen \emph{Bedarfsmeldung} in einer Liste aufgeteilt. Dabei wird jeder Listeneintrag mit der \emph{stopword}-Liste von \emph{nltk} verglichen. Stimmt das Wort nicht mit einem Eintrag der \emph{stopwords} überein, wird diese in die Liste \emph{words} hinzugefügt. Zum Schluss werden die Wörter wieder zu einem String zusammengetragen und zurückgegeben. \\

Um weitere Formatierungen und ungewünschte Zeichen zu entfernen wird die Bibliothek \emph{re} verwendet. Diese kann Regular Expression-Patterns anwenden und Bereiche, die zum Pattern passen entfernen.
%\begin{lstlisting}[caption={Implementation der Methode removeTags() und removeSpecialCharactersAndDigits() des Moduls \emph{preprocessing.py}}, label=lst:re]
%	import re
%	def removeTags(text):
%		return re.sub("</?.*?>"," <> ",text)
%	
%	def removeSpecialCharactersAndDigits(text):
%		return re.sub("(\\d|\\W)+"," ",text)
%\end{lstlisting}
%Im Listing \ref{lst:re} sind Implementationsdetails zur Entfernung von Tags und ungewollte Zeichen dargestellt. 
Die Methode \lstinline{removeTags()} erhält die Expression \lstinline{</?.*?>}. Dabei werden \lstinline{<Tags>} ermittelt und mit der Methode \lstinline{sub()} entfernt. Zur Entfernung von Ziffern und Nicht-Alphanummerischen Zeichen wird die Expression \lstinline{(\\d|\\W)+} angewendet. Als Ergebnis des Preprocessing wird ein gesäuberter String ohne Zeichen und Tags zurückgegeben.
\paragraph{TF-IDF}\mbox{}\\
Vorbereitend für die \emph{TF-IDF}-Methode wurden die \emph{Skills} und \emph{Aufgaben} aus allen \emph{Bedarfsmeldungen} ins Englische übersetzt, Preprocessed, in Kleinbuchstaben umgewandelt und in einem String zusammengefasst. Der Grund dafür ist, dass für die \emph{TF-IDF}-Methode ein Textkorpus benötigt wird, woraus die Schlüsselwörter durch \emph{Term Frequency} ermittelt werden. Damit dieser Prozess nur einmal erfolgen muss, wurden die Ergebnisse in die \url{requirements/preprocessedRequirements.json} gespeichert. Für die Implementierung des \emph{TF-IDF} wurden die Bibliotheken \emph{sklearn} und \emph{numpy} verwendet. Der Textkorpus wird dem \emph{TfidfVectorizer} von \emph{sklearn} beigefügt und die \emph{TF-IDF}-Werte werden berechnet. Anschließend werden alle durchschnittlichen \emph{TF-IDF}-Werte für jedes Wort im gesamten Textkorpus berechnet. Daraus wird eine Liste mit Wörtern und ihren durchschnittlichen \emph{TF-IDF}-Werten erstellt und in absteigender Reihenfolge sortiert. Durch ein vordefinierten Score Threshold können darunterliegende Schlüsselwörter entfernt werden. Dies ist wichtig, damit Wörter mit niedrigem Scoring und diese Beispielsweise nur einmal im Textkorpus auftauchen nicht als Schlüsselwörter erfasst werden. Als Ergebnis wird eine Liste mit Schlüsselwörtern zurückgegeben.

%\begin{lstlisting}[caption={Implementation des Moduls \emph{tfIdf.py}}, label=lst:postagging]
%import modules.readRequirements as readRequirements
%from sklearn.feature_extraction.text import TfidfVectorizer
%import numpy as np
%def useTfIdf(text):
%	object = readRequirements.loadJson("preprocessedRequirements.json")
%	documents = [object['tasks'], object['skills']]
%	vectorizer = TfidfVectorizer()
%	tfidf_matrix = vectorizer.fit_transform(documents)
%	feature_names = vectorizer.get_feature_names_out()
%	avg_tfidf_scores = np.mean(tfidf_matrix.toarray(), axis=0)
%	tfidf_scores = list(zip(feature_names, avg_tfidf_scores))
%	sorted_tfidf_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)
%	score_threshold = 0.1
%	filtered_keywords = [word for word, score in sorted_tfidf_scores if score > score_threshold]
%	return filtered_keywords
%\end{lstlisting}

%\paragraph{TextRank}\mbox{}\\
%Für die Implementation von \emph{TextRank} wurde die Bibliothek \emph{spaCy} und \emph{pytextrank} verwendet. Dieses bietet die NLP-Pipeline \emph{en\_core\_web\_sm}, das mit Internettext vortrainiert wurde und Vokabeln, Syntax und Entitäten enthält.
%\begin{lstlisting}[caption={Implementation des Moduls \emph{textRankingAlgorithm.py}}, label=lst:textrank]
%	import spacy
%	import pytextrank
%	def useTextRank(requirement):
%		nlp = spacy.load("en_core_web_sm")
%		nlp.add_pipe("textrank")
%		tokenized = nlp(requirement)
%		for phrase in tokenized._.phrases:
%			print(phrase.text)
%			print(phrase.rank, phrase.count)
%			print(phrase.chunks)
%\end{lstlisting}
%Das Listing \ref{textrank} zeigt die Implementierung des \emph{textRankingAlgorithm.py} Moduls. In Zeile 3 wird die \emph{Bedarfsmeldung} als Parameter übergeben. Die Pipeline \emph{en\_core\_web\_sm} wird in Zeile 4 geladen und in Zeile 5 werden die \emph{TextRank} Elemente der Pipeline hinzugefügt. Anschließend wird in Zeile 6 die \emph{Bedarfsmeldung} der Pipeline eingefügt und in Tokens umgewandelt.\\
%\todo{muss noch fertiggestellt werden}\\
\paragraph{N-Gramm}\mbox{}\\
Die Methode der \emph{n-Gramme} wurde mit der Bibliothek \emph{nltk} implementiert.
%\begin{lstlisting}[caption={Implementation des Moduls \emph{nGram.py}}, label=lst:ngram]
%	from nltk import ngrams
%	n = 2
%	def useNGram(text):
%		nGramList = []
%		nGrams = ngrams(text.split(), n)
%		for grams in nGrams:
%			nGramList.append(grams)
%		return nGramList
%\end{lstlisting}
%Das Listing \ref{lst:ngram} zeigt die Implementation des Moduls \emph{nGram.py}. 
Eine Variable \emph{n} ist definiert, die die Größe eines \emph{n-Gramms} widerspiegelt. Da in der Pipeline \emph{Bi-Gramme} benötigt werden, liegt der Wert von n auf 2. Mit der Methode \lstinline{ngrams()}
können die \emph{n-Gramme} generiert werden. Diese erhalten einen \emph{String} und die Variable \emph{n} als Parameter. Der zu überführende Text wird als Parameter übergeben und durch die Methode \lstinline{split()}
in einer List auf die einzelnen Wörter umgeformt. Die einzelnen Tupel mit den \emph{Bi-Grammen} werden am Ende zurückgegeben.
\paragraph{Entfernung von Bi-Grammen ohne Schlüsselwörter}\mbox{}\\
Zur Entfernung von Bi-Gramm-Tupel, die keine Schlüsselwörter enthalten wurde die Methode \lstinline{containsKeywords()} erstellt.
\begin{lstlisting}[caption={Implementation der Filterung für Schlüsselwörter in einer Bi-Gramm Liste}, label=lst:bigram]
	def containsKeywords(biGramList, keywordsList):
		filteredBiGram = []
		for tupel in biGramList:
			if any(word in tupel for word in keywordsList):
				filteredBiGram.append(tupel)
		return filteredBiGram
\end{lstlisting}
Das Listing \ref{lst:bigram} implementiert diese Methode. Als Parameter wird die \emph{Bi-Gramm}-Liste und die Schlüsselwörterliste übergeben. In Zeile 3 ist zu sehen, dass jeder Tupel traversiert wird. Dabei wird in Zeile 4 jedes Wort im Tupel mit der Schlüsselwörterliste verglichen. Wenn das aktuelle Wort in der Liste enthalten ist, so wird dieser Tupel in eine neue Liste aufgenommen. Diese wird am Ende zurückgegeben, wodurch eine gefilterte Liste mit ausschließlich enthaltenen Schlüsselwörtern vorhanden ist.
\paragraph{POS-Tagging}\mbox{}\\
In der Englischen Sprache existieren verschiedene Wortgruppenkombinationen, die zusammen ungewöhnlich klingen und dadurch beim sprechen und schreiben nicht oder nur selten verwendet werden.
\begin{center}
	\begin{tabularx}{1\textwidth} { 
			| >{\raggedright\arraybackslash}X 
			| >{\raggedright\arraybackslash}X
			| >{\raggedright\arraybackslash}X | }
		\hline
		Wortarten & POS-Tagging Abkürzungen & Beispiel \\
		\hline
		\hline
		Nomen + Adjektiv & NN JJ & "time beautiful"\\
		\hline
		Verb + Adjektiv & VB JJ & "think beautiful"\\
		\hline
		Verb + Pronomen & VB PRP & "think him"\\
		\hline
		Verb + Verb & VB VB & "think understand"\\
		\hline
		Adjektiv + Adjektiv & JJ JJ & beautiful strange\\
		\hline
		Adjektiv + Verb & JJ VB & beautiful think\\
		\hline
		Adjektiv + Adverb & JJ RB & "beautiful fast"\\
		\hline
		Adverb + Adjektiv & RB JJ & "fast beautiful"\\
		\hline
	\end{tabularx}\\
	\captionof{table}{Untypische Wortartenkombinationen in der englischen Sprache.}
	\label{tab:wortkombinationen}
\end{center}
In der Tabelle \ref{tab:wortkombinationen} sind einige Beispiele für solche ungewöhnlichen Zusammensetzungen. In Spalte eins sind die jeweiligen Wortarten beschrieben. Die zweite Spalte zeigt die dazugehörigen Tags, wie diese mit \emph{POS-Tagging} ermittelt werden. Die dritte Spalte zeigt jeweils ein Beispiel dieser Wortkombinationen. Auch wenn es weitere Kombinationen geben könnte, wurden nur die aus der Tabelle \ref{tab:wortkombinationen} im System verwendet.\\

Zur Implementierung der Methode \emph{POS-Tagging} wurde die Bibliothek \emph{nltk} verwendet. Die \emph{Tags} werden mit dem vortrainierten Modell \emph{averaged\_perceptron\_tagger} ermittelt.
%\begin{lstlisting}[caption={Implementation des Moduls \emph{posTagging.py}}, label=lst:postagging]
%	import nltk
%	nltk.download('averaged_perceptron_tagger')
%	def usePosTagging(text):
%		tokens = nltk.word_tokenize(text)
%		pos_tags = nltk.pos_tag(tokens)
%		return pos_tags
%\end{lstlisting}
%Im Listing \ref{lst:postagging} sind Details zur Implementierung dargestellt. 
Als Parameter wird ein String übergeben, der zwei Wörter aus den \emph{Bi-Grammen} enthält. Dieser String wird in Tokens umgewandelt. Diese werden in die Methode \lstinline{pos_tag()}
übergeben und eine Liste mit Tupeln, bei dem das Wort und das dazugehörige \emph{Tag} enthalten ist wird zurückgegeben.
%\paragraph{NER}\mbox{}\\
%Um die Methode \emph{NER} zu Implementieren wurde die Bibliothek \emph{spaCy} und dem Modell \emph{en\_core\_web\_sm} verwendet. 
%\begin{lstlisting}[caption={Implementation des Moduls \emph{ner.py}}, label=lst:ner]
%	import spacy
%	nlp = spacy.load("en_core_web_sm")
%	ner_categories = ["ORG","FAC","GPE","PRODUCT", "EVENT", "LANGUAGE", "DATE", "QUANTITY"]
%	def useNER(requirement):
%		tokenized = nlp(requirement)
%		entities = []
%		for ent in tokenized.ents:
%			if ent.label_ in ner_categories:
%				entities.append((ent.text, ent.label_))
%	return entities
%\end{lstlisting}
%Im Listing \ref{lst:ner} ist die Implementierung des Moduls \emph{ner.py} abgebildet. In Zeile 1 und 2 wird \emph{spaCy} importiert und das Modell \emph{en\_core\_web\_sm} geladen. In Zeile 3 werden die relevanten Kategorien von Wörtern definiert, die aus den \emph{Bedarfsmeldungen} extrahiert werden sollen. Die Kategorien sind Organisation (ORG), Einrichtung (FAC), Ort (GPE), Produkt (PRODUCT), Event (EVENT), Sprache (LANGUAGE), Datum (DATE) und Menge (QUANTITY). Innerhalb der Methode \lstinline{useNER()} wird die Bedarfsmeldung als Parameter übergeben und in Zeile 5 zu Tokens umgeformt. Anschließend werden alle Tokens durchlaufen und nach ihren Kategorien überprüft. Ist ein Token in einer der definierten Kategorien enthalten, wird der Text und die Kategorie in einem Tupel in der Liste \emph{entities} gespeichert und zurückgegeben.
\paragraph{Entfernung von Bi-Grammen mit ungewöhnlichen Wortartenkombinationen}\mbox{}\\
Die \emph{Bi-Gramm}-Liste muss im nächsten Schritt weiter reduziert werden. Hierbei werden alle \emph{POS-Tagging}-Kombinationen aus der Tabelle \ref{tab:wortkombinationen} mit den Tupeln aus der \emph{Bi-Gramm}-Liste verglichen werden.
\begin{lstlisting}[caption={Implementation der Filterung von Wortartenkombinationen}, label=lst:wortarten]
	combinationsToRemove = ["NN JJ", "VB JJ", "VB PRP", "VB VB", "JJ JJ", "JJ VB", "JJ RB", "RB JJ"]
	def removeWordCombination(biGramList):
		filteredList = []
		for left, right in biGramList:
			biGramString = f"{left} {right}"
			biGramStringsWithTags = usePosTagging(biGramString)
			tagString = ' '.join(tag for _, tag in biGramStringsWithTags)
			if tagString not in combinationsToRemove:
				filteredList.append((left, right))
		return filteredList
\end{lstlisting}
Das Listing \ref{lst:wortarten} implementiert diesen Schritt. Dazu werden in Zeile 1 alle Kombinationen als String in eine Liste zwischengespeichert. Als Parameter wird die zu filternde \emph{Bi-Gramm}-Liste übergeben. Diese wird in Zeile 4 traversiert und die Tupel mit den beiden Wörtern aus den \emph{Bi-Grammen} wird in Zeile 5 als String zusammengefügt. Dieser String wird in die \emph{POS-Tagging}-Methode übergeben und als Rückgabewert erhält das System eine Liste mit zwei Tupeln. Diese Tupel beinhalten jeweils eines der Wörter und das dazugehörige Tag aus dem \emph{POS-Tagging}. Die beiden Tags werden in Zeile 7 zusammengefügt, sodass ein String entsteht, wie es in der \emph{combinationsToRemove}-Liste in Zeile 1 ist. In Zeile 8 wird überprüft ob die Kombination aus Tags einer aus der Liste in Zeile 1 entspricht. Ist dies nicht der Fall, wird der Tupel in eine neue Liste eingefügt, die am Ende als Ausgabe zurückgegeben wird.
\paragraph{Überführung von Bi-Grammen in Stichpunkte}\mbox{}\\
Da der Volltext aus den Bedarfsmeldungen nicht einzelne Wörter, sondern inhaltlich aufeinander bezogene Stichpunkte entstehen sollen, werden die um die Schlüsselwörter herumliegenden Wörter aneinander gefügt. Dazu werden diejenigen \emph{Bi-Gramme} zu einem Satz zusammengefügt, die im ersten Tupel auf der rechten Seite und im zweiten Tupel auf der linken Seite das gleiche Wort enthalten. Hinzu kommt eine Überprüfung ob das darauffolgende linke Wort ein Schlüsselwort ist. Somit werden nur die Wörter zu Sätzen verkettet, die aus Schlüsselwörtern bestehen. In dem Fall, bei dem im darauf folgenden Tupel kein Schlüsselwort auf der linken sondern nur auf der rechten besteht, entsteht ein neuer Stichpunkt. Als Beispiel können folgende \emph{Bi-Gramme} betrachtet werden: (\grqq expertise\grqq, \grqq aws\grqq) (\grqq aws\grqq, \grqq technologie\grqq) (\grqq technologie\grqq, \grqq pflicht\grqq) (\grqq pflicht\grqq, \grqq englisch\grqq) (\grqq englisch\grqq, \grqq skills\grqq). Als Schlüsselwörter wurden die Wörter \grqq aws\grqq, \grqq pflicht\grqq, \grqq englisch\grqq ermittelt. Die Idee ist es, die zusammengehörigen \emph{Bi-Gramme} an diesen Schlüsselwörtern zusammenzufügen. Daraus werden die Stichpunkte \grqq expertise aws technologie\grqq und \grqq technologie pflicht englisch skills\grqq
\begin{lstlisting}[caption={Umformung der Bi-Gramm Liste in Stichpunkte}, label=lst:stichpunkte]
	def combineWords(filteredBiGram, keywords):
		combinedWords = []
		i = 0
		while i < len(filteredBiGram):
			keyPhrase = recursivCombine(filteredBiGram, i, "", keywords, "")
			combinedWords.append(keyPhrase)
			wordsCount = len(keyPhrase.split(" "))
			wordsCount = max(1, wordsCount - 1)
			i += wordsCount
			if(i > len(filteredBiGram)):
				break
		return combinedWords
	
	def recursivCombine(filteredBiGram, index, currentString, keywords, lastRightItem):
		left, right = filteredBiGram[index]
		if not currentString:
			currentString = left
		else:
			currentString += " " + left
		if lastRightItem in {left, ""}:
			if right in keywords and index + 1 < len(filteredBiGram):
				currentString = recursivCombine(filteredBiGram, index + 1, currentString, keywords, right)
			else:
				currentString += " " + right
		return currentString
\end{lstlisting}
Die Implementierung zu dieser Idee ist im Listing \ref{lst:stichpunkte} dargestellt. Die Methode \lstinline{combineWords} in Zeile 1 erhält die \emph{Bi-Gramm}-Liste und die Schlüsselwörterliste als Parameter. Die \emph{Bi-Gramm}-Liste wird traversiert und ein string aus zusammengehörigen Wörtern wird rekursiv in Zeile 5 mit der Methode \lstinline{recursivCombine} erstellt. Dazu wird in jedem rekursiven Schritt überprüft, ob das darauf folgende rechte wort ein Schlüsselwort und das gleiche wie das linke Wort im aktuellen Tupel entspricht. 

\section{Erklärung des Systems anhand eines Beispiels}
\todo{Muss ich noch machen}\\
Dieser Schritt kann in diesem Beispiel besser verstanden werden:\\
\begin{enumerate}
	\item Zu diesem Zeitpunkt haben wir ein relevantes Wort aus der TF-IDF-Methode: \grqq aws\grqq
	\item In diesem Beispiel haben wir die \emph{Bi-Gramme}: (\grqq expert\grqq, \grqq aws\grqq) und (\grqq aws\grqq, \grqq technologies\grqq)
	\item Da \grqq aws\grqq ein relevantes Wort ist und beim ersten \emph{Bi-Gramm} rechts und beim zweiten links steht, wird diese Reihe an \emph{Bi-Grammen} zusammengefügt.
	\item Das Ergebnis ist \grqq expert aws technologies\grqq
\end{enumerate}