%\section{Verwandte Arbeiten}
\chapter{Literaturüberblick}
\label{sec:literaturueberblick}
Es gibt eine Reihe an verwandten Arbeiten, die sich mit unterschiedlichen Aspekten des \emph{Staffing}-Prozesses und der Nutzung von Information Retrieval und Filtering zur Informationsgewinnung beschäftigen. Dennoch beschäftigt sich keine Arbeit mit dem spezifischen Problem der Informationsgewinnung aus \emph{Bedarfsmeldungen}, in der Art, wie sie \emph{adesso} verwendet. Es wird ein Einblick in die Art und Weise gegeben wie andere Autoren Information Retrieval einsetzen und kombinieren.\\
\section{Techniken}
Nachfolgend werden Details zu verschiedenen Arbeiten absatzweise dargestellt. Die Quellen stehen jeweils am Anfang jedes Absatzes und beziehen sich aufgetroffene Aussagen innerhalb der Absätze. Diese befassen sich mit unterschiedlichen Techniken, die Lösungen für die Implementierungsebene des zu entwickelnden Systems bieten.
\subsection*{Automatisiertes Staffing}
In der Arbeit \citetitle{horesh2016information}\cite{horesh2016information} beschreiben \citeauthor{horesh2016information} einen Ansatz zur Ableitung von Unternehmensdaten und digitalen Fußabdrücken von Mitarbeitern. Mit Hilfe eines Big-Data-Workflows, der die Komponenten Information Retrieval und Suche, Datenfusion, Matrixvervollständigung und ordinale Regression nutzt, können Informationen zur Expertise automatisch zusammengeführt und für die Nutzung durch Experten aufbereitet werden. Das System soll Fähigkeiten, Talente und Fachwissen der Mitarbeiter in einem breiten Bereich wie cloud computing oder cybersecurity einschätzen. Beim Ansatz des Information Retrieval und -fusion wird eine Liste von Suchbegriffen erstellt, die sich auf das breite Fachgebiet der Mitarbeiter beziehen. Die Suche wird nach jedem dieser Abfragebegriffe durchgeführt, um Zusammenhänge zwischen Mitarbeiter und Datenquellen zu finden. Die verschiedenen Zusammenhänge werden zusammengefügt, gewichtet und nach der Abfrage sortiert. Die Mitarbeiter werden nach Daten gewichtet und bewertet, um einen einzigen Wert (sehr niedrig, niedrig, moderat, etwas, begrenzt) für ihr Fachwissen in diesem breiten Bereich zu erhalten.\\
\subsection*{Vorverarbeitung}
Die Autoren \citeauthor{jain2013data} der Arbeit \citetitle{jain2013data}\cite{jain2013data} beschreiben Data-Mining als ein interdisziplinäres Teilgebiet der Informatik, das sich mit der rechnergestützten Entdeckung von Mustern in großen Datenbeständen befasst \cite{jain2013data}. Ziel dieses fortgeschrittenen Analyseverfahrens ist es, Informationen aus einem Datensatz zu extrahieren und in eine für die weitere Verwendung verständliche Struktur umzuwandeln \cite{jain2013data}. Die verwendeten Methoden liegen an der Schnittstelle zwischen künstlicher Intelligenz, maschinellem Lernen, Statistik, Datenbanksystemen und Business Intelligence \cite{jain2013data}. Beim Data Mining geht es um die Lösung von Problemen durch die Analyse von Daten, die bereits in Datenbanken vorhanden sind \cite{jain2013data}.\\

In der Arbeit \citetitle{alasadi2017review}\cite{alasadi2017review} zeigen die Autoren \citeauthor{alasadi2017review} Wege und Schritte zur Aufbereitung von Datensätzen auf. Die Arbeit umfasst Data-Mining Vorverarbeitungsmethoden, um die Qualität der Daten zu verbessern. Diese weisen wichtige Schritte auf, um die Effizienz in der Datensammlung zu verbessern. Die Datenvorverarbeitung (preprocessing) stellt eine der essenziellen Data-Mining-Aufgaben dar, die die Vorbereitung und Umwandlung von Daten in eine geeignete Form umfasst. Die Datenvorverarbeitung zielt unter anderem darauf ab, die Datenmenge zu reduzieren und Daten zu standardisieren. Die Datenvorverarbeitung umfasst eine Reihe von Techniken, darunter Datenbereinigung, -integration, -transformation und -reduktion. \\

In der Arbeit mit dem Titel \citetitle{kroha2000preprocessing}\cite{kroha2000preprocessing} von dem Autor \citeauthor{kroha2000preprocessing} wird der Teil des Anforderungsspezifikationsprozesses diskutiert, der zwischen der textuellen Anforderungsdefinition und den dazugehörigen Diagrammen der Anforderungsspezifikation liegt. Es wird die These aufgestellt, dass die Erstellung einer textuellen Anforderungsbeschreibung, die das Verständnis des Analysten für das Problem darstellt, die Effizienz der Anforderungsvalidierung durch den Benutzer verbessert. Die vorliegende Idee ist laut dem Autor aus dem Problem entstanden, dass Software-Entwickler nicht immer über die erforderlichen Kenntnisse in den fachlichen Abläufen der Themengebiete verfügen, die für die Erstellung der Software relevant sind. Im Rahmen der Anforderungsdefinition erfolgt eine textuelle Verfeinerung, die als Anforderungsbeschreibung bezeichnet werden kann. Bei der Arbeit mit dem unterstützten Werkzeug \emph{Tessi} ist der Analytiker durch die genannten Vorgaben gezwungen, Anforderungen zu vervollständigen und zu erklären sowie die Rollen der Wörter im Text im Sinne der objektorientierten Analyse zu spezifizieren. Im Rahmen der Vorverarbeitung erfolgt eine Transformation der Requirements durch Templates.
\subsection*{Schlüsselwörter identifizieren}
In der Arbeit \citetitle{ramos2003using}\cite{ramos2003using} haben die Autoren \citeauthor{ramos2003using} die \emph{TF-IDF}-Methode (\emph{Term Frequency-Inverse Document Frequency}) zur Ermittlung der Häufigkeit von Wörtern in einem bestimmten Dokument im Vergleich zum Anteil dieses Wortes im gesamten Dokumenten ermittelt. Die Berechnung erlaubt eine Einschätzung der Relevanz eines bestimmten Wortes in einem bestimmten Dokument. Die Grundidee des Ansatzes besteht darin, dass Wörter, die in einem einzigen Dokument oder in einer kleinen Gruppe von Dokumenten häufig vorkommen, tendenziell höhere \emph{TF-IDF}-Werte aufweisen als häufig vorkommende Wörter wie Artikel und Präpositionen. \emph{TF-IDF} stellt laut den Autoren ein effizientes Verfahren zum Abgleich von Wörtern in einer Anfrage mit Dokumenten dar. Bei Eingabe einer Abfrage zu einem bestimmten Thema durch einen Benutzer kann \emph{TF-IDF} relevante Informationen zu dieser Abfrage in Dokumenten finden. Trotz der Stärken von \emph{TF-IDF}, sind auch seine Grenzen zu berücksichtigen. In Bezug auf Synonyme ist zu beachten, dass \emph{TF-IDF} nicht auf die Beziehung zwischen den Wörtern eingeht. Des Weiteren werden laut den Autoren unterschiedliche Schreibweisen von Wörtern nicht berücksichtigt, was dazu führen kann, dass Wörter fälschlicherweise als nicht so häufig auftauchend deklariert werden, obwohl sie mit leicht abgewandelter Schreibweise häufiger vorkommen.\\
Die Autoren haben die \emph{TF-IDF}-Methode anhand von 1400 Dokumenten getestet. Dazu wurden die \emph{TF-IDF}-Werte berechnet und die ersten 100 Dokumente zurückgegeben. Die zurückgegebenen Dokumente werden in absteigender Reihenfolge zurückgegeben, wobei die Dokumente mit höheren Gewichtssummen zuerst erscheinen. Um die Ergebnisse zu vergleichen, wurde zu mehreren Dokumenten die Anzahl einer bestimmten Anfrage ermittelt.\\

Die Autoren \citeauthor{bafna2016document} der Arbeit \citetitle{bafna2016document}\cite{bafna2016document} haben im Rahmen der Textanalyse die \emph{TF-IDF}-Methode angewandt, um häufige Begriffe zu eliminieren und lediglich die relevantesten Begriffe aus einem Textkorpus zu extrahieren. In der Untersuchung wird der \emph{TF-IDF}-Algorithmus zusammen mit dem \emph{Fuzzy K-means} und dem \emph{hierarchischen} Algorithmus verwendet. In einem ersten Schritt werden Experimente mit einem kleinen Datensatz durchgeführt und eine Clusteranalyse vorgenommen. Im Anschluss erfolgt die Anwendung des vorher besten ermittelten Algorithmus auf den erweiterten Datensatz. In Kombination mit den verschiedenen Clustern der verwandten Dokumente werden der resultierende \emph{Silhouettenkoeffizient}, die \emph{Entropie} sowie der \emph{F1-Score} dargestellt, um das Verhalten des Algorithmus für den Datensatz zu veranschaulichen.\\
\subsection*{Wortketten bilden}
Die Autoren \citeauthor{majumder2002n} beschreiben in der Arbeit \citetitle{majumder2002n}\cite{majumder2002n} \emph{N-Gramme} als Folgen von Zeichen oder Wörtern, die aus einem Text extrahiert werden. Diese lassen sich in zwei Kategorien unterteilen: i) zeichenbasiert und ii) wortbasiert. Ein Zeichen-\emph{N-Gramm} bezeichnet eine Folge von n aufeinanderfolgenden Zeichen, die aus einem Wort extrahiert werden. Die Hauptmotivation hinter diesem Ansatz besteht darin, dass ähnliche Wörter einen hohen Anteil an \emph{N-Grammen} gemeinsam haben werden. In der Regel umfasst ein \emph{N-Gramm} lediglich die am häufigsten auftretenden Wortpaare und verwendet einen Backoff-Mechanismus, um die Wahrscheinlichkeit zu berechnen, die bei der Suche nach dem gewünschten Wortpaar nicht erfolgreich war. Die Analyse von \emph{N-Grammen} erlaubt die Identifikation häufig vorkommender Phrasen oder Begriffe, die als potenzielle Schlüsselwörter bezeichnet werden können. In der Durchführung des Experimentes werden von den Autoren \emph{N-Gramme} verwendet, um die indische Sprache aus mehrsprachigen Dokumentensammlungen zu identifizieren. Dazu werden zunächst \emph{N-Gramm}-Profile für 10 indische Sprachen erstellt. Ein \emph{N-Gramm}-Häufigkeitsprofil wird durch Zählen aller \emph{N-Gramme} in einer Reihe von Dokumenten in einer bestimmten Sprache und deren Sortierung in absteigender Reihenfolge erstellt. Im Falle der Identifizierung einer neuen Dokumentsprache wird ein \emph{N-Gramm}-Profil des Dokuments erstellt und anschließend der Abstand zwischen dem neuen Dokumentprofil und den Sprachprofilen berechnet. Der Abstand wird mit dem \emph{out-of-place measure} zwischen den beiden Profilen berechnet. Der kürzeste Abstand wird ausgewählt und es wird vorhergesagt, ob das bestimmte Dokument zu dieser Sprache gehört. Zur Vermeidung einer Fehlklassifizierung wurde ein Schwellenwert eingeführt, bei dessen Überschreitung das System die Aussage trifft, dass die Sprache des Dokuments nicht bestimmt werden kann.
\subsection*{Wortgruppen identifizieren}
Die Autoren \citeauthor{kumawat2015pos} der Arbeit \citetitle{kumawat2015pos}\cite{kumawat2015pos} beschreiben unterschiedliche Ansätze von \emph{POS-Tagging} (\emph{Part-of-Speech-Tagging}), um einen für indische Sprache zu erstellen. Sie beschreiben, dass die Katalogisierung von Wortarten (\emph{POS}) einen Prozess bezeichnet, bei dem jedem einzelnen Wort eines Satzes ein Wortart-Tag oder ein anderes philologisches Klassenzeichen zugeordnet wird. Die Vorverarbeitungsaufgabe des \emph{Taggings} von Sprachbestandteilen stellt laut den Autoren einen essenziellen Schritt in \emph{NLP} dar.\\

\emph{NLP} (Natural Language Processing) stellt einen zentralen Aspekt im Bereich der künstlichen Intelligenz sowie der Computerwissenschaften dar \cite{kang2020natural}. Studien in diesem Bereich umfassen Theorien und Methoden, die eine Kommunikation zwischen Menschen und Computern in natürlicher Sprache ermöglichen \cite{kang2020natural}. \emph{NLP} vereint die Gebiete Informatik, Linguistik und Mathematik mit dem primären Ziel, menschliche Sprache in Befehle zu übersetzen, die von Computern ausgeführt werden können \cite{kang2020natural}.\\

Die Zuordnung von Wortarten stellt laut den Autoren \citeauthor{kumawat2015pos} der Arbeit \citetitle{kumawat2015pos}\cite{kumawat2015pos} eine grundlegende Aufgabe bei der Verarbeitung natürlicher Sprache dar. Die Erstellung erfolgt unter Zuhilfenahme linguistischer Theorien, zufälliger Muster sowie einer Kombination aus beidem. Ein \emph{POS-Tagger} ist definiert als ein Teil einer Software, der jedem Wort einer Sprache, das er liest, eine Wortart zuordnet. Die Ansätze des \emph{POS-Tagging} lassen sich in drei Kategorien unterteilen: \emph{regelbasiertes Tagging}, \emph{statistisches Tagging} und \emph{hybrides Tagging}. Im Rahmen der Zuweisung von \emph{POS-Tags} zu Wörtern im regelbasierten \emph{POS}-System erfolgt die Verwendung einer Reihe von handgeschriebenen Regeln in Kombination mit Kontextinformationen. Der Nachteil dieses Systems besteht darin, dass es nicht funktioniert, wenn der Text nicht bekannt ist. Das Problem besteht darin, dass das System nicht in der Lage ist, den passenden Text vorherzusagen. Um eine höhere Effizienz und Genauigkeit in diesem System zu erreichen, ist es daher empfehlenswert, einen umfassenden Satz von handkodierten Regeln zu verwenden. Die Häufigkeit und Wahrscheinlichkeit sind in dem statistischen Ansatz einbezogen. Der grundlegende statistische Ansatz basiert auf der am häufigsten verwendeten Markierung für ein bestimmtes Wort in den annotierten Trainingsdaten. Diese Information wird auch zur Markierung dieses Wortes im Text verwendet.\\
Das Experiment aus der Arbeit umfasst einen Datensatz aus 20000 Sätzen mit manuellen Tags in der Sprache Marathi. Damit kann das entwickelte \emph{POS-Tagging}-System getestet werden. 

%\section{Text-Ranking-Algorithmus}
%Text-Ranking-Algorithmen: Text-Ranking-Algorithmen wie TextRank oder YAKE (Yet Another Keyword Extractor) verwenden graphenbasierte Methoden, um Schlüsselwörter in einem Text zu identifizieren. Die Algorithmen bewerten die Wichtigkeit von Wörtern basierend auf ihrer Verbindung zu anderen Wörtern im Text und extrahieren Schlüsselwörter entsprechend ihrer Rangfolge.\\ \cite{mihalcea2004textrank}\cite{zhang2020empirical}\cite{pay2019ensemble}\\

%Die Autoren \citeauthor{mihalcea2004textrank} verwenden in der Arbeit \citetitle{mihalcea2004textrank}\cite{mihalcea2004textrank} die Methode \emph{TextRank}, um Schlüsselwörter in einem natürlichen Text zu ermitteln. Dabei handelt es sich um ein graphenbasierten Text-Ranking-Algorithmus, der eine Möglichkeit darstellt, die Wichtigkeit eines Knotens innerhalb eines Graphen zu bestimmen. Dabei werden globale Informationen berücksichtigt, die rekursiv aus dem gesamten Graphen berechnet werden. Im Gegensatz zu anderen Methoden, die sich lediglich auf lokale, knotenspezifische Informationen stützen, ermöglicht dies eine objektivere Bewertung. Der \emph{TextRank}-Algorithmus zur Schlüsselwortextraktion
%ist ein \emph{unsupervised}-Ansatz. Zunächst wird der Text im Vorverarbeitungsschritt \emph{tokenisiert} und mit \emph{Part-of-Speech-Tags} annotiert, der für die Anwendung syntaktischer Filter notwendig ist. Um ein Wachstum des Graphen durch Hinzufügen aller Kombinationen von Sequenzen, die aus mehr als einem \emph{N-Gramm} bestehen, zu vermeiden, werden nur einzelne Wörter zum Hinzufügung in den Graphen in Betracht gezogen. Anschließend werden alle lexikalischen Einheiten, in den Graphen eingefügt und es wird eine Kante zwischen den lexikalischen Einheiten eingefügt. Nach der Erstellung des Graphen wird jedem Knoten eine Punktzahl mit einen Anfangswert von 1 gegeben. Der Ranking-Algorithmus wird anschließend in mehreren Iterationen auf den Graphen angewendet. Im Anschluss an die Ermittlung der Punktzahlen für jeden Knoten im Graphen erfolgt eine Sortierung der Knoten in umgekehrter Reihenfolge ihrer Punktzahl. Die Knoten mit der höchsten Punktzahl werden für die Nachbearbeitung ausgewählt. Bei Nachbearbeitung werden alle vom \emph{TextRank}-Algorithmus als potenzielle Schlüsselwörter im Text markiert. Anschließend werden Sequenzen benachbarter Schlüsselwörter zusammengefasst.\\

%\cite{zhang2020empirical}\cite{pay2019ensemble}
\subsection*{Datum und Zeitangaben identifizieren}
% \cite{nadeau2007survey}\cite{partalidou2019design}\\
Die Arbeit \citetitle{mansouri2008named}\cite{mansouri2008named} der Autoren \citeauthor{mansouri2008named} vergleichen verschiedene \emph{NER}-Ansätze (Named Entity Recognition) auf die Genauigkeit, um die Stärken und Schwächen einzelner Methoden zu identifizieren. \emph{NER} stellt einen Teilbereich der Informationsextraktion dar und umfasst die Verarbeitung sowohl strukturierter als auch unstrukturierter Dokumente und die Identifizierung von Wörtern, die sich auf Personen, Orte, Organisationen und Unternehmen beziehen. \emph{NER} stellt eine grundlegende Aufgabe eines Systems im Bereich des \emph{NLP} dar. \emph{NER} umfasst laut den Autoren zwei Aufgaben. Die erste Aufgabe besteht in der Identifizierung von Eigennamen im Text. Die zweite Aufgabe ist die Klassifizierung dieser Namen in eine Reihe von vordefinierten Kategorien. Dazu zählen (i)Personennamen, (ii)Organisationen, wie Unternehmen, Regierungsorganisationen, Ausschüsse usw., (iii)Orte, wie Städte, Länder, Flüsse usw., (iv)Datumsangaben und (v)Zeitangaben.\\
%Die Arbeit vergleicht Ansätze mithilfe von den Methoden \emph{Precision}, \emph{Recall} und \emph{F1-Score}
\subsection*{Hybride Ansätze}
In der Untersuchung \citetitle{croft2000combining}\cite{croft2000combining} wird von den Autoren \citeauthor{croft2000combining} die Entwicklung von Kombinationen im Bereich des Information Retrievals analysiert. Dabei werden sowohl experimentelle Ergebnisse als auch die Retrieval-Modelle, die als formale Rahmen für die Kombination vorgeschlagen wurden, berücksichtigt. In der Untersuchung wird aufgezeigt, dass Kombinationsansätze für die Informationssuche als Kombination der Ergebnisse mehrerer Klassifikatoren auf der Grundlage einer oder mehrerer Darstellungen modelliert werden können. Zudem wird dargelegt, dass dieses einfache Modell Erklärungen für viele der experimentellen Ergebnisse liefern kann.\\

Die Arbeit \citetitle{chiny2021lstm}\cite{chiny2021lstm} von den Autoren \citeauthor{chiny2021lstm} kombiniert drei Ansätze des Information Retrievals mit dem Ziel, relevante Informationen aus Produktreviews zu extrahieren. Der Ansatz \emph{TF-IDF} wird mit einem sogenannten \emph{CLASSIFIER} Model kombiniert. Das Klassifikationsmodell verarbeitet drei Eingaben der Modelle \emph{LSTM}, \emph{VADER} und \emph{TF-IDF}. Die Werte dieser Eingaben liegen im Bereich von [0,1]. Die Ausgabe des Klassifikationsmodells ist binär und gibt eine Vorhersage des vollständigen Textes der Modelleingabe aus (positiv oder negativ). Aus einem Datensatz wurden 5000 zufällige Bewertungen ausgewählt, die sich von den für die LSTM- und \emph{TF-IDF}-Modelle verwendeten Trainings- und Testdatensätzen unterscheiden. Die Autoren \citeauthor{chiny2021lstm} haben das System durch die Eingabe des globalen Modells laufen lassen, um die Vorhersagen zu erhalten, die von den \emph{LSTM}-, \emph{VADER}- und \emph{TF-IDF}-Modellen zurückgegeben werden. Anschließend teilten sie diese Ergebnisse in zwei Stapel auf (75 \% für die Trainingsmenge und 25 \% für die Testmenge), um das binäre Klassifizierungsmodell zu trainieren und zu bewerten. Die Evaluation erfolgt durch den Einsatz der Methoden \emph{Precision}, \emph{Recall} und \emph{F1-Score}.\\

Die Arbeit \citetitle{suhasini2021hybrid}\cite{suhasini2021hybrid} von den Autoren \citeauthor{suhasini2021hybrid} befasst sich mit der Filterung von Falschmeldungen. In diesem Beitrag werden hybride Verfahren zur Gewinnung von Merkmalen untersucht, die in dem Gebiet noch nicht gründlich erforscht wurden. Die Anwendung von Hybridsystemen hat sich in einer Vielzahl von Anwendungsbereichen als nützlich erwiesen und zeigen eine Tendenz, die Fehlerquote zu reduzieren, indem sie Techniken wie \emph{TF-IDF} und \emph{N-Grams} verwenden. Es wurden Experimente unter Verwendung von Echtzeit-Twitterdaten durchgeführt. Der Datensatz umfasste ca. 5.800 Tweets, die sich auf Donald-Drummond-Geschichten bezogen. Die Sammlung und Verarbeitung der Tweets erfolgt mit Python. Der Datensatz umfasste Original-Tweets, die als gefälscht und echt gekennzeichnet wurden. Die Genauigkeit der Prognose wurde anhand der verschiedenen Nachrichten evaluiert, die für das Training verwendet wurden und am Ende mit \emph{Precision}, \emph{Recall} und \emph{F1-Score} evaluiert.\\

Im Rahmen der Studie mit dem Titel \citetitle{darmawan2015hybrid}\cite{darmawan2015hybrid} wurde von den Autoren \citeauthor{darmawan2015hybrid} ein hybrider Algorithmus zur Extraktion von Schlüsselwörtern und Kosinusähnlichkeit zur Verbesserung der Satzkohäsion bei der Textzusammenfassung vorgeschlagen. Die vorgeschlagene Methode basiert auf einer Komprimierung von 50 \%, 30 \% und 20 \%, um Kandidaten für die Zusammenfassung zu erstellen. Die Auswertung des Ergebnisses mittels \emph{t-Test} zeigt, dass die vorgeschlagene Methode den Kohäsionsgrad signifikant erhöht.\\
Der Ablauf umfasst die Analyse eines Dokuments mithilfe eines Extraktionsalgorithmus sowie die Berechnung der \emph{TF-IDF}-Werte für jeden Begriff. Anschließend werden alle \emph{TF-IDF}-Werte für jeden Satz summiert. Im nächsten Schritt werden alle Sätze anhand der Summe von \emph{TF-IDF} eingestuft. Das Kompressionsverhältnis bestimmt die Position des Satzrangs. In dieser Studie wird eine Kompression von 50 \% verwendet, was bedeutet, dass die Satzzusammenfassung um 50 \% des Originaltextes gekürzt wird. Nach der Auswahl des Satzes wird dessen Berechnung durchgeführt. Die Ähnlichkeit wird mit der \emph{Cosinus-Ähnlichkeitsmethode} berechnet. Anschließend werden alle Sätze anhand ihrer \emph{Cosinus-Ähnlichkeit} von der höchsten zur niedrigsten sortiert. Der resultierende Text mit neuer Satzanordnung stellt die finale Zusammenfassung dar.\\

\subsection*{Pipeline}
In der Arbeit \citetitle{pirk2019implementierung} \cite{pirk2019implementierung} wird von dem Autor \citeauthor{pirk2019implementierung} eine Pipeline entwickelt, die die \emph{N-Gramm}-Analyse verwendet, um Schlagwörter aus einem Text zu extrahieren und mit verschiedenen Ansätzen von Word-Clouds zu visualisieren. Der Fokus dieser Studie liegt dennoch eher auf der Visualisierung als auf der Informationsgewinnung eines Textes.\\

Die Arbeit \citetitle{lavin2019analyzing}\cite{lavin2019analyzing} von dem Autor \citeauthor{lavin2019analyzing} präsentiert eine Anleitung zur Erstellung einer Pipeline mit Python und \emph{TF-IDF}. Darüber hinaus wird die Relevanz von \emph{TF-IDF} als Vorverarbeitung beim maschinellen Lernen erörtert. Im Vergleich zur rohen Termhäufigkeit weist \emph{TF-IDF} in der Regel einen höheren Vorhersagewert auf. Die Gewichtung von Themenwörtern wird erhöht, um die Bedeutung von Wörtern zu erhöhen, während die Gewichtung von hochfrequenten Funktionswörtern verringert wird. Es werden Verfahren zur Vorverarbeitung von Texten vorgestellt, die eine Umformung in die gewünschte Darstellungsform ermöglichen. Zudem werden Methoden zur Interpretation der Ergebnisse des \emph{TF-IDF}-Verfahrens erörtert. Die vorliegende Arbeit widmet sich zunächst einer detaillierten Betrachtung der zugrundeliegenden Algorithmen und ihrer Funktionsweise. Im Anschluss erfolgt die Implementierung in Python. Die Verwendung der Bibliothek \emph{sklearn} ist dabei von zentraler Bedeutung.\\

In dem Beitrag mit dem Titel \citetitle{partalidou2019design}\cite{partalidou2019design} wird von den Autoren \citeauthor{partalidou2019design} ein maschineller Lernansatz für die Bereiche \emph{POS-Tagging} und \emph{NER} für die griechische Sprache unter Verwendung von \emph{spaCy} erarbeitet und evaluiert. Die Verarbeitung natürlicher Sprache wirft insbesondere bei der Analyse unüblicher Sprachen wie Griechisch Schwierigkeiten auf.\\ Der Datensatz wurde aus Texten einer griechischen Zeitung extrahiert. Die Artikel der Zeitung wurden in verschiedene Kategorien wie beispielsweise Sport, Gesundheit, Wirtschaft und politische Nachrichten eingeteilt. Die Daten bestehen aus einer Reihe von XML-Dateien, die Informationen auf der Ebene von Absätzen, Sätzen und Wörtern enthalten. Im Rahmen der Evaluation wurden verschiedene Parameter getestet, um das optimale Ergebnis zu erzielen. Der Datensatz wurde gemischt und in einen Trainingssatz, einen Testsatz und einen Validierungssatz aufgeteilt. Zur Evaluierung wurden die Methoden des \emph{Precision}, \emph{Recalls} und \emph{F1-Scores} angewendet.

\section{Ergebnis}
In der Literatur finden sich verschiedene Ansätze zur Extraktion relevanter Stichpunkte aus einem Volltext. Eine Methode zur Ermittlung wichtiger Schlüsselwörter in Texten stellt die \emph{TF-IDF}-Methode dar. Innerhalb einer oder mehrerer \emph{Bedarfsmeldungen} lassen sich mit dieser Methode häufig auftauchende Wörter ermitteln. Es besteht somit die Möglichkeit, Wörter aus einer \emph{Bedarfsmeldung} mit anderen \emph{Bedarfsmeldungen} zu vergleichen und die Häufigkeit der Wörter zu berechnen, um somit potenzielle Schlüsselwörter zu ermitteln. %Des Weiteren können graphenbasierte Methoden wie \emph{TextRank} zur Identifizierung von Schlüsselwörtern in Texten herangezogen werden. 
Ein Ansatz zur Bildung von Wortketten ist die Nutzung von \emph{N-Grammen}. Dabei lassen sich Sätze und Absätze zur weiteren Verarbeitung separieren, wodurch Wörter und ihre Zusammenhänge identifizieren lassen. Auch grammatische Kategorien von Wörtern können Rückschlüsse auf potenzielle Schlüsselwörter zulassen. Die \emph{POS-Tagging}-Methode stellt eine Möglichkeit dar, um dieses Ziel zu erreichen. Des Weiteren kann \emph{NER} dazu beitragen Zeitangaben zu identifizieren. Hybride, die sich durch besondere Zusammensetzungen auszeichnen, zeigen Überlegenheiten gegenüber den einzelnen Ansätzen, die für sich genommen jeweils nur eine Teilkompetenz abdecken.

%spam-filter (Empfinde das Thema eventuell als zu unpassend)\\
%-Überblick über verfügbare Methoden, Herausforderungen und zukünftige Forschungsrichtungen im Bereich der Spam-Erkennung, Filterung und Eindämmung von SMS-Spam. Dabei werden auch Methodiken der keyword frequency ratio und Herunterbrechung auf keyword components behandelt \cite{shafi2017review}\\

%In diesem Beitrag werden Studien zu Technologien vorgestellt, die für die Suche und das Abrufen von Informationen im Web nützlich sind. Es wird aufgezeigt, dass Information Retrieval und Ranking im Web-Kontext anders Funktioniert als in einer statischen Datenbank. \cite{kobayashi2000information}\\

%konzeptbasiertes recruiting --> neuer Ansatz

%\section{Relevante Methoden und Techniken im Bereich Information Retrieval und Data-Mining}
%\label{sec:relevante-methoden}
\newpage





