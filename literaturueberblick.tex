\chapter{Grundlagen}
\label{chap:literaturüberblick}
In diesem Kapitel werden die für das Thema notwendigen Grundlagen und bereits erforschten Themengebiete im Kontext von Recommender Systemen und Informationsverarbeitung behandelt, die für das weitere Verständnis der Arbeit notwendig sind. Es wird ein Einblick in die Art und Weise gegeben, wie andere Autoren Information Retrieval und Filtering einsetzen und kombinieren.
\section{Kontext}
Als IT-Dienstleister wird \emph{adesso} von Kunden unter anderem mit der Entwicklung individueller Softwarelösungen beauftragt. Derzeit verbringen Führungskräfte jedoch viel Zeit damit, interne Mitarbeiterinnen und Mitarbeiter manuell für Kundenprojekte zu suchen und diese dann aufgrund ihrer Erfahrungen und Fähigkeiten auszuwählen und entsprechend einzusetzen. Dieser Prozess soll durch eine KI-Lösung unterstützt werden. Da es sich bei der Personalsuche um einen geschäftskritischen Prozess handelt, ist der Spielraum für Fehler gering. Im internen Projekt \emph{adesso Staffing Advisor} wird eine durch Large Language Model-gestützte Anwendung entwickelt, die Führungskräfte bei der Suche nach geeignetem Personal für ausgewählte Projekte unterstützt. Der Ansatz des Large Language Modeling ist jedoch nicht deterministisch. Es besteht die Gefahr, dass bei gleichem Input unterschiedliche Ergebnisse erzielt werden. Daher versucht \emph{adesso} durch den Einsatz von Methoden und Technologien neben dem Large Language Model-Ansatz deterministische Ergebnisse zu erzielen, die auf einem ähnlichen Niveau liegen.\\

Die vorliegende Ausarbeitung befasst sich mit der Informationsgewinnung in \emph{Bedarfsmeldungen}. Eine \emph{Bedarfsmeldung} bezeichnet eine Projektbeschreibung eines Kunden, die Anforderungen an ein zu entwickelndes System enthält. Die Erstellung der \emph{Bedarfsmeldung} erfolgt durch den Kunden, wobei eine gemeinsame Abstimmung mit \emph{adesso} zur Finalisierung und Speicherung im JIRA erfolgt. Die Informationen sind insofern unstrukturiert, als dass sie ohne vordefinierte Struktur in Form eines Volltexts vorliegen. Infolgedessen kann es zu Abweichungen hinsichtlich der Ausgestaltung von \emph{Bedarfsmeldungen} kommen.\\

\section{Bedarfsmeldungen}
Die Problemstellung umfasst eine Reihe von Punkten, die im Rahmen der Ausarbeitung zu behandeln sind. Aufgrund der unstrukturierten und mit fehlenden Informationen versehenen \emph{Bedarfsmeldungen} ist eine Standardisierung dieser \emph{Bedarfsmeldungen} von besonderer Relevanz. Dies würde die Extraktion relevanter Informationen erleichtern und somit die Effizienz des Systems verbessern. Zudem würde ein solcher Ansatz einen Einblick in die Relevanz von Informationen und Stichpunkten geben. Im Rahmen der weiteren Bearbeitung einer Standardisierung ist die Extraktion der erforderlichen Informationen aus dem Volltext erforderlich. In diesem Kontext existiert bereits eine Reihe an Methoden und Ansätzen, die sich in der Forschung bewährt haben. Der unstrukturierte Volltext muss in eine strukturierte inhaltliche Aufteilung in einzelne Sequenzen und Stichpunkte überführt werden.\\

In der Literatur finden sich verschiedene Ansätze zur Extraktion wichtiger Stichpunkte aus einem Volltext. Eine Methode zur Ermittlung wichtiger Stichpunkte in Texten stellt die \emph{tf-idf}-Methode (Term Frequency-Inverse Document Frequency) dar. Innerhalb einer oder mehrerer \emph{Bedarfsmeldungen} lassen sich mit dieser Methode häufig auftauchende Wörter ermitteln. Des Weiteren können graphenbasierte Methoden wie \emph{TextRank} oder \emph{YAKE} (Yet Another Keyword Extractor) zur Identifizierung von Schlüsselwörtern in Texten herangezogen werden. Ein weiterer Ansatz ist die Nutzung von \emph{N-Grammen}, die häufig vorkommende Phrasen oder Begriffe identifizieren können. Auch grammatische Kategorien von Wörtern können Rückschlüsse auf potenzielle Schlüsselwörter zulassen. Die \emph{POS-Tagging}-Methode (Part-of-Speech-Tagging) stellt eine Möglichkeit dar, um dieses Ziel zu erreichen. Des Weiteren kann \emph{NER} (Named Entity Recognition) dazu beitragen, Personen, Firmennamen, Orte, Ereignisse oder Zeitangaben zu identifizieren. Auch wenn die \emph{Bedarfsmeldungen} unstrukturiert sind, haben sich im Laufe der Zeit Konventionen entwickelt, die teilweise Strukturen eines \emph{Patterns} aufweisen können.

\section{Recommender Systems Historie und aktueller Stand der Forschung}
Auch wenn die Erstellung eines Recommender Systems nicht Gegenstand der vorliegenden Ausarbeitung ist, stellt die Nutzung von Information Retrieval und Filtering ein entscheidener Schritt in Richtung eines funktionierenden Recommender Systems dar. Das Verständnis der Funktionsweise eines Recommender Systems sowie dessen Entwicklung in den vergangenen Jahren ist daher für das Verständnis des Teilbereichs dieser Thematik von Nutzen.\\

Recommender Systems existieren bereits seit vielen Jahren. Im Jahr 1992 führten Belkin und Croft eine Analyse und einen Vergleich des Information Retrievals und Filtering durch \cite{dong2022brief}. Das Information Retrieval behandelt die grundlegende Technologie der Suchmaschine \cite{dong2022brief}. Das Recommender System basiert hauptsächlich auf der Technologie des Information Filtering. Im selben Jahr präsentierte Goldberg das Tapestry-System, welches das erste System zur Informationsfilterung darstellt, das auf kollaboratives Filtern durch menschliche Bewertung basiert. Die Mehrheit der frühen Empfehlungsmodelle basiert auf kollaborativer Empfehlungen, wobei K-Nearest-Neighbor (KNN)-Modelle eine besondere Rolle einnehmen. Diese Modelle prognostizieren die Nachbarn eines Zielnutzers, indem sie eine Ähnlichkeit zwischen den vorherigen Präferenzen und den Präferenzen der anderen Nutzer berechnen \cite{dong2022brief}. Die Studie von Goldberg inspirierte einige Forscher des Massachusetts Institute of Technology (MIT) und der University of Minnesota (UMN) dazu, einen Nachrichtenempfehlungsdienst mit dem Namen \emph{GroupLens} zu entwickeln. Die Hauptkomponente dieses Dienstes ist ein Modell zur kollaborativen Filterung zwischen Nutzern \cite{dong2022brief}. Das gleichnamige Forschungslabor kann somit als Pionier auf dem Gebiet der Recommender Systems bezeichnet werden. Die dort durchgeführten Forschungen bilden die Grundlage für nachfolgende Musik- und Video-Ähnlichkeitsempfehlungen \cite{dong2022brief}. \\

Recommender Systeme haben in den letzten Jahren verschiedene Definitionen erhalten. Eine dieser Definitionen wird in dem Artikel von Resnick und Varian (1997) sinngemäß so beschrieben, dass ein typisches Recommender System Empfehlungen durch Personen als Eingabe erhält, die das System dann zusammenschließt und an geeignete Empfänger weiterleitet \cite{burke2011recommender}. In einigen Fällen besteht die primäre Transformation in der Zusammenführung, in anderen Fällen liegt die Fähigkeit des Systems darin, gute Übereinstimmungen zwischen Empfehlungsgebern und Empfehlungsempfängern herzustellen \cite{burke2011recommender}. Empfehlungssysteme stellen ein Instrument zur Interaktion mit umfangreichen und vielschichtigen Informationen dar. Sie ermöglichen eine personalisierte Sicht auf diese Informationen, indem sie die für den Nutzer wahrscheinlich relevanten Inhalte aufbereiten \cite{burke2011recommender}. Besonders im Handelsverkehr im Internet sind Recommender Systeme ein häufiger Einsatzgebiet. Dabei werden Recommender Systeme als Werkzeuge zum Suchen und Filtern von Informationen verwendet, die dem Benutzer Vorschläge unterbreiten, die für ihn nützlich sein könnten. Sie sind in einer Vielzahl von Internetanwendungen weit verbreitet und helfen den Nutzern, bessere Entscheidungen bei der Suche nach Nachrichten, Musik, Urlaubsangeboten oder Geldanlagen zu treffen \cite{ricci2014recommender}. Eine spezifisches Recommender System konzentriert sich normalerweise auf eine Art von Themengebiet wie z. B. Filme oder Nachrichten \cite{ricci2014recommender}. Darüber hinaus sind sie zu einem entscheidenden Faktor in der Entscheidungsfindung von Organisationen geworden \cite{chartron2014general}. Unternehmen wie \emph{adesso} bauen immer weiter auf Recommender System unterstützte System auf, um Prozesse zu beschleunigen oder zu vereinfachen. Grundsätzlich können die Methoden in die Typen (i)\emph{collaborative Filtering-based} (kollaborative Empfehlungssysteme), (ii)\emph{content-based} (inhaltsbasierte Empfehlungssysteme), (iii)\emph{knowledge-based} (wissensbasiert Empfehlungssysteme) und (iv)\emph{hybrid} (hybride Empfehlungssysteme) unterteilt werden.\\

Jede Empfehlungsmethode hat ihre Vorteile und Grenzen \cite{lu2020recommender}. Insbesondere das inhaltsbasierte Empfehlungssystem bring eine hohe Relevanz für das Mitarbeiterempfehlungssystem. Die Grundprinzipien inhaltsbasierter Empfehlungssysteme sind zum einen die Analyse der Beschreibung der von einem bestimmten Benutzer bevorzugten \emph{Items}, um die gemeinsamen Hauptattribute (Präferenzen) zu identifizieren, die diese \emph{Items} unterscheiden. Diese Präferenzen werden in einem \emph{Benutzerprofil} gespeichert \cite{lu2020recommender}. Zusätzlich werden die Eigenschaften jedes \emph{Items} mit dem \emph{Benutzerprofil} verglichen, so dass nur \emph{Items} empfohlen werden, die eine hohe Ähnlichkeit mit dem \emph{Benutzerprofil} aufweisen \cite{lu2020recommender}. Bei der Idee der Mitarbeiterempfehlung kann also die \emph{Bedarfsmeldung} mit den benötigten Projektskills und Anforderung als \emph{Benutzerprofil} angesehen werden. Die Mitarbeiterprofile sind dabei die \emph{Items}. Die Attribute werden verglichen (Skills der Mitarbeiter mit den Skills und Anforderungen der \emph{Bedarfsmeldung}) und ähnliche \emph{Items} werden vorgeschlagen. Mit Hilfe traditioneller Methoden des Information Retrievals, wie z.B. dem Kosinus-Ähnlichkeitsmaß, werden dann Empfehlungen generiert \cite{lu2020recommender}. Darüber hinaus generieren sie Empfehlungen mit Hilfe von statistischen und maschinelle Lernverfahren, die in der Lage sind, Nutzerinteressen aus historischen Nutzerdaten zu lernen \cite{lu2020recommender}.
\section{Information Retrieval und Information Filtering}
%Diese Arbeit beschreibt den Unterschied zwischen Information Filtering und Information Retrieval\cite{belkin1992information}
Im Allgemeinen wird einem Informationssystem die Funktion zugeschrieben, den Benutzer zu den Dokumenten zu führen, die seinen Informationsbedarf am besten decken. Allgemeiner ausgedrückt ist das Ziel eines Informationssystems, dem Benutzer Informationen aus der Wissensressource zur Verfügung zu stellen, die ihm helfen, ein Problem zu lösen.Auf der anderen Seite ist unter Filtern das Entfernen von Daten aus einem eingehenden Datenstrom zu versteht und nicht das Auffinden von Daten in diesem Datenstrom. Filtersysteme verarbeiten große Datenmengen. Typische Anwendungen betreffen Gigabytes von Text oder weitaus größere Mengen anderer Medien. Während es bei dem Information Retrieval typischerweise um die einmalige Nutzung des Systems durch eine Person mit einem einmaligen Ziel und einer einmaligen Anfrage geht, befasst sich die Informationsfilterung mit der wiederholten Nutzung des Systems durch eine oder mehrere Personen mit langfristigen Zielen oder Interessen.\cite{belkin1992information}
\section{Data-Mining}
Data Mining ist ein interdisziplinäres Teilgebiet der Informatik, das sich mit der rechnergestützten Entdeckung von Mustern in großen Datenbeständen befasst. Ziel dieses fortgeschrittenen Analyseverfahrens ist es, Informationen aus einem Datensatz zu extrahieren und in eine für die weitere Verwendung verständliche Struktur umzuwandeln. Die verwendeten Methoden liegen an der Schnittstelle zwischen künstlicher Intelligenz, maschinellem Lernen, Statistik, Datenbanksystemen und Business Intelligence. Beim Data Mining geht es um die Lösung von Problemen durch die Analyse von Daten, die bereits in Datenbanken vorhanden sind.\cite{jain2013data}

%\section{Verwandte Arbeiten}
\chapter{Literaturüberblick}
%\label{sec:forschung-und-ansätze}
Es gibt eine Reihe an verwandten Arbeiten die sich mit unterschiedlichen Aspekten des \emph{Staffing}-Prozesses und der Nutzung von Information Retrieval und Filtering zur Informationsgewinnung beschäftigen. Dennoch beschäftigt sich keine Arbeit mit dem spezifischen Problem der Informationsgewinnung aus \emph{Bedarfsmeldungen}.

\section{Automatisiertes Staffing}
In der ersten Arbeit mit dem Titel \citetitle{horesh2016information}\cite{horesh2016information} beschreiben die Autoren einen Ansatz zur Ableitung von Unternehmensdaten und digitalen Fußabdrücken von Mitarbeitern. Mit Hilfe eines Big-Data-Workflows, der die Komponenten Information Retrieval und Suche, Datenfusion, Matrixvervollständigung und ordinale Regression nutzt, können Informationen zur Expertise automatisch zusammengeführt und für die Nutzung durch Experten aufbereitet werden. Das System soll Fähigkeiten, Talente und Fachwissens der Mitarbeiter in einem breiten Bereich wie cloud computing oder cybersecurity einschätzen. Beim Ansatz des Information Retrieval und -fusion wird eine Liste von Suchbegriffen erstellt, die sich auf das breite Fachgebiet der Mitarbeiter beziehen. Die Suche wird nach jedem dieser Abfragebegriffe durchgeführt, um Zusammenhänge zwischen Mitarbeiter und Datenquellen zu finden. Die verschiedenen Zusammenhänge werden miteinander zusammengefügt, gewichtet und nach der Abfrage sortiert. Die Mitarbeiter werden nach Daten gewichtet und bewertet, um einen einzigen Wert (sehr niedrig, niedrig, moderat, etwas, begrenzt) für ihr Fachwissen in diesem breiten Bereich zu erhalten.\\

Auch wenn diese Arbeit in eine ähnliche Richtung geht, können die einzelnen Schritte nicht auf die Situation der Informationsgewinnung einer \emph{Bedarfsmeldung} übertragen werden. \emph{Bedarfsmeldungen} können nicht nur auf spezifische Bereiche herunter reduziert werden und das ist auch nicht das Ziel... 


%\subsection{Recommender System}
%Content based recommendation für filme über genres \cite{reddy2019content}\\

\section{Information Filtering}
Die Arbeit mit dem Titel \citetitle{lanquillon2001enhancing}\cite{lanquillon2001enhancing} befasst sich unter anderem mit dem Aspekt des content based Information Filtering. Das Ziel dabei ist es Informationen auf die Interessengebiete der Benutzer zu reduzieren. Dazu werden nicht relevante Dokumente aus einem Strom von Informationen entfernt, sodass dem Anwendern nur relevante Dokumente präsentiert werden. Ein Teil der Arbeit beschäftigt sich mit der Informationsfilterung und mögliche Filterungsvarianten werden vorgestellt. Die Arbeit konzentriert sich auf die inhaltsbasierte Filterung von Textdokumenten und identifiziert Informationsfilterung als einen Spezialfall der Textklassifikation. Dazu wird ein Überblick über gängige Methoden des Information Filtering gegeben und ihre Leistung evaluiert.

\section{Vorverarbeitung}
\todo{gucken ob ich das drin lassen will}
Diese Arbeit zeigt Wege und Schritte zur Aufbereitung von Datensätzen auf. Die Arbeit umfasst Data-Mining Vorverarbeitungsmethoden, um die Qualität der Daten zu verbessern. Diese weisen wichtiger Schritte auf, um die Effizienz in der Datensammlung zu verbessern \cite{alasadi2017review}. (Nicht sicher ob ich das drin lassen soll)\\

In der Arbeit mit dem Titel \citetitle{kroha2000preprocessing}\cite{kroha2000preprocessing} wird der Teil des Anforderungsspezifikationsprozesses diskutiert, der zwischen der textuellen Anforderungsdefinition und den dazugehörigen Diagrammen der Anforderungsspezifikation liegt. Es wird die These aufgestellt, dass die Erstellung einer textuellen Anforderungsbeschreibung, welche das Verständnis des Analysten für das Problem darstellt, die Effizienz der Anforderungsvalidierung durch den Benutzer verbessert. Die vorliegende Idee ist aus dem Problem entstanden, dass Software-Entwickler nicht immer über die erforderlichen Kenntnisse in den fachlichen Abläufen der Themengebiete verfügen, die für die Erstellung der Software relevant sind. Im Rahmen der Anforderungsdefinition erfolgt eine textuelle Verfeinerung, welche als Anforderungsbeschreibung bezeichnet werden kann. Bei der Arbeit mit dem unterstützten Werkzeug \emph{Tessi} ist der Analytiker durch die genannten Vorgaben gezwungen, Anforderungen zu vervollständigen und zu erklären sowie die Rollen der Wörter im Text im Sinne der objektorientierten Analyse zu spezifizieren. Im Rahmen der Vorverarbeitung erfolgt eine Transformation der Requirements durch Templates.

\section{Hybride Ansätze}
In der Untersuchung mit dem Titel \citetitle{croft2000combining}\cite{croft2000combining} wird die Entwicklung von Kombinationen im Bereich des Information Retrievals analysiert. Dabei werden sowohl experimentelle Ergebnisse als auch die Retrieval-Modelle, die als formale Rahmen für die Kombination vorgeschlagen wurden, berücksichtigt. Es wird aufgezeigt, dass Kombinationsansätze für die Informationssuche als Kombination der Ergebnisse mehrerer Klassifikatoren auf der Grundlage einer oder mehrerer Darstellungen modelliert werden können. Zudem wird dargelegt, dass dieses einfache Modell Erklärungen für viele der experimentellen Ergebnisse liefern kann.\\

Die Arbeit mit dem Titel \citetitle{chiny2021lstm}\cite{chiny2021lstm} kombiniert drei Ansätze des Information Retrievals mit dem Ziel, relevante Informationen aus Produktreviews zu extrahieren. Der Ansatz TF-IDF wird mit einem sogenannten CLASSIFIER Model kombiniert. Das Klassifikationsmodell verarbeitet drei Eingaben der Modelle LSTM, VADER und TF-IDF. Die Werte dieser Eingaben liegen im Bereich von [0,1]. Die Ausgabe des Klassifikationsmodells ist binär und gibt eine Vorhersage des vollständigen Textes der Modelleingabe aus (positiv oder negativ). Aus einem Datensatz wurden 5000 zufällige Bewertungen ausgewählt, die sich von den für die LSTM- und TF-IDF-Modelle verwendeten Trainings- und Testdatensätzen unterscheiden. Die Autoren haben sie durch die Eingabe des globalen Modells laufen lassen, um die Vorhersagen zu erhalten, die von den Modellen LSTM-, VADER- und TF-IDF-Modelle zu erhalten. Anschließend teilten sie diese Ergebnisse in zwei Stapel auf (75 \% für die Trainingsmenge und 25 \% für die Testmenge), um das binäres Klassifizierungsmodell zu trainieren und zu bewerten. Die Evaluation erfolgt durch den Einsatz der Methoden Precision, Recall und F1-Score.\\

Die Arbeit mit dem Titel \citetitle{suhasini2021hybrid}\cite{suhasini2021hybrid} befasst sich mit der Filterung von Fake news. In diesem Beitrag werden hybride Verfahren zur Gewinnung von Merkmalen untersucht, die in dem Gebiet noch nicht gründlich erforscht wurden. Die Anwendung von Hybridsystemen hat sich in einer Vielzahl von Anwendungsbereichen als nützlich erwiesen und zeigen eine Tendenz, die Fehlerquote zu reduzieren, indem sie Techniken wie TF-IDF und N-Grams verwenden. Es wurden Experimente unter Verwendung von Echtzeit-Twitterdaten durchgeführt. Der Datensatz umfasste ca. 5.800 Tweets, die sich auf Donald-Drummond-Geschichten bezogen. Die Sammlung und Verarbeitung der Tweets erfolgt mit Python. Der Datensatz umfasste Original-Tweets, die als gefälscht und echt gekennzeichnet wurden. Die Genauigkeit der Prognose wurde anhand der verschiedenen Nachrichten evaluiert, die für das Training verwendet wurden und am Ende mit Precision, Recall und F1-Score evaluiert.\\

Im Rahmen der Studie mit dem Titel \citetitle{darmawan2015hybrid}\cite{darmawan2015hybrid} wurde ein hybrider Algorithmus zur Extraktion von Schlüsselwörtern und Kosinusähnlichkeit zur Verbesserung der Satzkohäsion bei der Textzusammenfassung vorgeschlagen. Die vorgeschlagene Methode basiert auf einer Komprimierung von 50 \%, 30 \% und 20 \%, um Kandidaten für die Zusammenfassung zu erstellen. Die Auswertung des Ergebnisses mittels t-Test zeigt, dass die vorgeschlagene Methode den Kohäsionsgrad signifikant erhöht.\\
Der Ablauf umfasst die Analyse eines Dokuments mithilfe eines Extraktionsalgorithmus sowie die Berechnung der TF/IDF-Werte für jeden Begriff. Anschließend werden alle TF/IDF-Werte für jeden Satz summiert. Im nächsten Schritt werden alle Sätze anhand der Summe von TF/IDF eingestuft. Das Kompressionsverhältnis bestimmt die Position des Satzrangs. In dieser Studie wird eine Kompression von 50 \% verwendet, was bedeutet, dass die Satzzusammenfassung um 50 \% des Originaltextes gekürzt wird. Nach der Auswahl des Satzes wird dessen Berechnung durchgeführt. Die Ähnlichkeit wird mit der Cosinus-Ähnlichkeitsmethode berechnet. Anschließend werden alle Sätze anhand ihrer Cosinus-Ähnlichkeit von der höchsten zur niedrigsten sortiert. Der resultierende Text mit neuer Satzanordnung stellt die finale Zusammenfassung dar.\\

\section{Pipeline}
In der Arbeit mit dem Titel \citetitle{pirk2019implementierung}\cite{pirk2019implementierung} Arbeit wird eine Pipeline entwickelt, die die N-Gramm-Analyse verwendet, um Schlagwörter aus einem Text zu extrahieren und mit verschiedenen Ansätzen von Word-Clouds zu visualisieren. Der Fokus dieser Studie liegt dennoch eher auf der Visualisierung als auf der Informationsgewinnung eines Textes.\\

Die Arbeit mit dem Titel \citetitle{lavin2019analyzing}\cite{lavin2019analyzing} präsentiert eine Anleitung zur Erstellung einer Pipeline mit Python und TF-IDF. Darüber hinaus wird die Relevanz von TF-IDF als Vorverarbeitung beim maschinellen Lernen erörtert. Im Vergleich zur rohen Termhäufigkeit weist TF-IDF in der Regel einen höheren Vorhersagewert auf. Die Gewichtung von Themenwörtern wird erhöht, um die Bedeutung von Wörtern zu erhöhen, während die Gewichtung von hochfrequenten Funktionswörtern verringert wird. Es werden Verfahren zur Vorverarbeitung von Texten vorgestellt, die eine Umformung in die gewünschte Darstellungsform ermöglichen. Zudem werden Methoden zur Interpretation der Ergebnisse des TF-IDF-Verfahrens erörtert. Die vorliegende Arbeit widmet sich zunächst einer detaillierten Betrachtung der zugrundeliegenden Algorithmen und ihrer Funktionsweise. Im Anschluss erfolgt die Implementierung in Python. Die Verwendung der Bibliothek \emph{sklearn} ist dabei von zentraler Bedeutung.\\

In dem Beitrag mit dem Titel \citetitle{partalidou2019design}\cite{partalidou2019design} wird ein maschineller Lernansatz für die Bereiche Part-of-Speech-Tagging und Named-Entity-Recognition für die griechische Sprache unter Verwendung von \emph{spaCy} erarbeitet und evaluiert. Die Verarbeitung natürlicher Sprache wirft insbesondere bei der Analyse unüblicher Sprachen wie Griechisch Schwierigkeiten auf.\\ Der Datensatz wurde aus Texten einer griechischen Zeitung extrahiert. Die Artikel der Zeitung wurden in verschiedene Kategorien wie beispielsweise Sport, Gesundheit, Wirtschaft und politische Nachrichten eingeteilt. Die Daten bestehen aus einer Reihe von XML-Dateien, die Informationen auf der Ebene von Absätzen, Sätzen und Wörtern enthalten. Im Rahmen der Evaluation wurden verschiedene Parameter getestet, um das optimale Ergebnis zu erzielen. Der Datensatz wurde gemischt und in einen Trainingssatz, einen Testsatz und einen Validierungssatz aufgeteilt. Zur Evaluierung wurden die Methoden der Präzision, des Recalls und des F1-Scores angewendet.

%spam-filter (Empfinde das Thema eventuell als zu unpassend)\\
%-Überblick über verfügbare Methoden, Herausforderungen und zukünftige Forschungsrichtungen im Bereich der Spam-Erkennung, Filterung und Eindämmung von SMS-Spam. Dabei werden auch Methodiken der keyword frequency ratio und Herunterbrechung auf keyword components behandelt \cite{shafi2017review}\\

%In diesem Beitrag werden Studien zu Technologien vorgestellt, die für die Suche und das Abrufen von Informationen im Web nützlich sind. Es wird aufgezeigt, dass Information Retrieval und Ranking im Web-Kontext anders Funktioniert als in einer statischen Datenbank. \cite{kobayashi2000information}\\

%konzeptbasiertes recruiting --> neuer Ansatz

%\section{Relevante Methoden und Techniken im Bereich Information Retrieval und Data-Mining}
%\label{sec:relevante-methoden}
\newpage





