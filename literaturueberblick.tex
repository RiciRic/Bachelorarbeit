%\section{Verwandte Arbeiten}
\chapter{Literaturüberblick}
%\label{sec:forschung-und-ansätze}
\todo{überall die Autoren mit Namen erwähnen?}\\
Es gibt eine Reihe an verwandten Arbeiten, die sich mit unterschiedlichen Aspekten des \emph{Staffing}-Prozesses und der Nutzung von Information Retrieval und Filtering zur Informationsgewinnung beschäftigen. Dennoch beschäftigt sich keine Arbeit mit dem spezifischen Problem der Informationsgewinnung aus \emph{Bedarfsmeldungen}, in der Art, wie sie die \emph{adesso} verwendet.

\section{Automatisiertes Staffing}
In der ersten Arbeit mit dem Titel \citetitle{horesh2016information}\cite{horesh2016information} beschreiben die Autoren \citeauthor{horesh2016information} einen Ansatz zur Ableitung von Unternehmensdaten und digitalen Fußabdrücken von Mitarbeitern. Mit Hilfe eines Big-Data-Workflows, der die Komponenten Information Retrieval und Suche, Datenfusion, Matrixvervollständigung und ordinale Regression nutzt, können Informationen zur Expertise automatisch zusammengeführt und für die Nutzung durch Experten aufbereitet werden. Das System soll Fähigkeiten, Talente und Fachwissens der Mitarbeiter in einem breiten Bereich wie cloud computing oder cybersecurity einschätzen. Beim Ansatz des Information Retrieval und -fusion wird eine Liste von Suchbegriffen erstellt, die sich auf das breite Fachgebiet der Mitarbeiter beziehen. Die Suche wird nach jedem dieser Abfragebegriffe durchgeführt, um Zusammenhänge zwischen Mitarbeiter und Datenquellen zu finden. Die verschiedenen Zusammenhänge werden miteinander zusammengefügt, gewichtet und nach der Abfrage sortiert. Die Mitarbeiter werden nach Daten gewichtet und bewertet, um einen einzigen Wert (sehr niedrig, niedrig, moderat, etwas, begrenzt) für ihr Fachwissen in diesem breiten Bereich zu erhalten.\\

Auch wenn diese Arbeit in eine ähnliche Richtung geht, können die einzelnen Schritte nicht auf die Situation der Informationsgewinnung einer \emph{Bedarfsmeldung} übertragen werden. \emph{Bedarfsmeldungen} können nicht nur auf spezifische Bereiche herunter reduziert werden und das ist auch nicht das Ziel... 


%\subsection{Recommender System}
%Content based recommendation für filme über genres \cite{reddy2019content}\\

\section{Information Filtering}
Die Arbeit mit dem Titel \citetitle{lanquillon2001enhancing}\cite{lanquillon2001enhancing} befasst sich unter anderem mit dem Aspekt des content based Information Filtering. Das Ziel dabei ist es, Informationen auf die Interessengebiete der Benutzer zu reduzieren. Dazu werden nicht relevante Dokumente aus einem Strom von Informationen entfernt, sodass dem Anwendern nur relevante Dokumente präsentiert werden. Ein Teil der Arbeit beschäftigt sich mit der Informationsfilterung und mögliche Filterungsvarianten werden vorgestellt. Die Arbeit konzentriert sich auf die inhaltsbasierte Filterung von Textdokumenten und identifiziert Informationsfilterung als einen Spezialfall der Textklassifikation. Dazu wird ein Überblick über gängige Methoden des Information Filtering gegeben und ihre Leistung evaluiert.

\section{Preprocessing}
\todo{gucken ob ich das drin lassen will}\\
Diese Arbeit zeigt Wege und Schritte zur Aufbereitung von Datensätzen auf. Die Arbeit umfasst Data-Mining Vorverarbeitungsmethoden, um die Qualität der Daten zu verbessern. Diese weisen wichtiger Schritte auf, um die Effizienz in der Datensammlung zu verbessern \cite{alasadi2017review}. (Nicht sicher ob ich das drin lassen soll)\\

In der Arbeit mit dem Titel \citetitle{kroha2000preprocessing}\cite{kroha2000preprocessing} wird der Teil des Anforderungsspezifikationsprozesses diskutiert, der zwischen der textuellen Anforderungsdefinition und den dazugehörigen Diagrammen der Anforderungsspezifikation liegt. Es wird die These aufgestellt, dass die Erstellung einer textuellen Anforderungsbeschreibung, welche das Verständnis des Analysten für das Problem darstellt, die Effizienz der Anforderungsvalidierung durch den Benutzer verbessert. Die vorliegende Idee ist aus dem Problem entstanden, dass Software-Entwickler nicht immer über die erforderlichen Kenntnisse in den fachlichen Abläufen der Themengebiete verfügen, die für die Erstellung der Software relevant sind. Im Rahmen der Anforderungsdefinition erfolgt eine textuelle Verfeinerung, welche als Anforderungsbeschreibung bezeichnet werden kann. Bei der Arbeit mit dem unterstützten Werkzeug \emph{Tessi} ist der Analytiker durch die genannten Vorgaben gezwungen, Anforderungen zu vervollständigen und zu erklären sowie die Rollen der Wörter im Text im Sinne der objektorientierten Analyse zu spezifizieren. Im Rahmen der Vorverarbeitung erfolgt eine Transformation der Requirements durch Templates.

\paragraph{Data-Mining}
Data-Mining: \cite{jun2001review}\cite{jain2013data}

preprocessing: \cite{garcia2016big}
\section{TF-IDF}
Im Rahmen der Textanalyse wird die TF-IDF-Technik angewendet, welche die häufigsten Begriffe eliminiert und lediglich die relevantesten Begriffe aus einem Textkorpus extrahiert \cite{bafna2016document}. Die TF-IDF-Methode dient der Ermittlung der Häufigkeit von Wörtern in einem bestimmten Dokument im Vergleich zum Anteil dieses Wortes im gesamten Dokumenten \cite{ramos2003using}. Die Berechnung erlaubt eine Einschätzung der Relevanz eines bestimmten Wortes in einem bestimmten Dokument \cite{ramos2003using}. Die Grundidee des Ansatzes besteht darin, dass Wörter, die in einem einzigen Dokument oder in einer kleinen Gruppe von Dokumenten häufig vorkommen, tendenziell höhere TF-IDF-Werte aufweisen als häufig vorkommende Wörter wie Artikel und Präpositionen \cite{ramos2003using}. TF-IDF stellt ein effizientes Verfahren zum Abgleich von Wörtern in einer Anfrage mit Dokumenten dar \cite{ramos2003using}. Bei Eingabe einer Abfrage zu einem bestimmten Thema durch einen Benutzer kann TF-IDF relevante Informationen zu dieser Abfrage in Dokumenten finden \cite{ramos2003using}. In Bezug auf die Bedarfsmeldungen besteht somit die Möglichkeit, Wörter aus einer Bedarfsmeldung mit anderen Bedarfsmeldungen zu vergleichen und die Häufigkeit ihrer Verwendung zu ermitteln, um somit potenzielle Schlüsselwörter zu ermitteln.\\

Trotz der Stärken von TF-IDF, sind auch seine Grenzen zu berücksichtigen. In Bezug auf Synonyme ist zu beachten, dass TF-IDF nicht auf die Beziehung zwischen den Wörtern eingeht. Des Weiteren werden unterschiedliche Schreibweisen von Wörtern nicht berücksichtigt, was dazu führen kann, dass Wörter fälschlicherweise als nicht so häufig auftauchend deklariert werden, obwohl sie mit leicht abgewandelter Schreibweise häufiger vorkommen.
\section{Text-Ranking-Algorithmen}
Text-Ranking-Algorithmen: Text-Ranking-Algorithmen wie TextRank oder YAKE (Yet Another Keyword Extractor) verwenden graphenbasierte Methoden, um Schlüsselwörter in einem Text zu identifizieren. Die Algorithmen bewerten die Wichtigkeit von Wörtern basierend auf ihrer Verbindung zu anderen Wörtern im Text und extrahieren Schlüsselwörter entsprechend ihrer Rangfolge.\\ \cite{mihalcea2004textrank}\cite{zhang2020empirical}\cite{pay2019ensemble}\\

Ein graphenbasierter Rangordnungsalgorithmus stellt eine Möglichkeit dar, die Wichtigkeit eines Knotens innerhalb eines Graphen zu bestimmen. Dabei werden globale Informationen berücksichtigt, die rekursiv aus dem gesamten Graphen berechnet werden. Im Gegensatz zu anderen Methoden, die sich lediglich auf lokale, knotenspezifische Informationen stützen, ermöglicht dies eine objektivere Bewertung.\cite{mihalcea2004textrank}

\section{N-Gramm}
N-Gramme sind Folgen von Zeichen oder Wörtern, die aus einem Text extrahiert werden. N-Gramme lassen sich in zwei Kategorien unterteilen: i) zeichenbasiert und ii) wortbasiert. Ein Zeichen-N-Gramm bezeichnet eine Folge von n aufeinanderfolgenden Zeichen, die aus einem Wort extrahiert werden. Die Hauptmotivation hinter diesem Ansatz besteht darin, dass ähnliche Wörter einen hohen Anteil an N-Grammen gemeinsam haben werden. In der Regel umfasst ein N-Gramm lediglich die am häufigsten auftretenden Wortpaare und verwendet einen Backoff-Mechanismus, um die Wahrscheinlichkeit zu berechnen, die bei der Suche nach dem gewünschten Wortpaar nicht erfolgreich war. \cite{majumder2002n} Die Analyse von N-Grammen erlaubt die Identifikation häufig vorkommender Phrasen oder Begriffe, die als potenzielle Schlüsselwörter bezeichnet werden können.

\section{POS-Tagging}
%Part-of-Speech (POS) Tagging: POS-Tagging wird genutzt, um die grammatischen Kategorien von Wörtern in einem Text zu bestimmen. Durch die Berücksichtigung von Wörtern mit bestimmten POS-Tags wie Substantiven oder Adjektiven können relevante Schlüsselwörter extrahiert werden.\\ \cite{kumawat2015pos}\cite{nakagawa2007hybrid}\\

Die Katalogisierung von Wortarten (POS) bezeichnet einen Prozess, bei dem jedem einzelnen Wort eines Satzes ein Wortart-Tag oder ein anderes philologisches Klassenzeichen zugeordnet wird. Die Vorverarbeitungsaufgabe des Taggings von Sprachbestandteilen stellt einen essenziellen Schritt in der Verarbeitung natürlicher Sprache dar. Die Zuordnung von Wortarten stellt eine grundlegende Aufgabe bei der Verarbeitung natürlicher Sprache dar. Die Erstellung erfolgt unter Zuhilfenahme linguistischer Theorien, zufälliger Muster sowie einer Kombination aus beidem. Ein Part-of-Speech-Tagger (POS-Tagger) ist definiert als ein Teil einer Software, der jedem Wort einer Sprache, das er liest, eine Wortart zuordnet. Die Ansätze des POS-Tagging lassen sich in drei Kategorien unterteilen: regelbasiertes Tagging, statistisches Tagging und hybrides Tagging. Im Rahmen der Zuweisung von POS-Tags zu Wörtern im regelbasierten POS-System erfolgt die Verwendung einer Reihe von handgeschriebenen Regeln in Kombination mit Kontextinformationen. Der Nachteil dieses Systems besteht darin, dass es nicht funktioniert, wenn der Text nicht bekannt ist. Das Problem besteht darin, dass das System nicht in der Lage ist, den passenden Text vorherzusagen. Um eine höhere Effizienz und Genauigkeit in diesem System zu erreichen, ist es daher empfehlenswert, einen umfassenden Satz von handkodierten Regeln zu verwenden. Die Häufigkeit und Wahrscheinlichkeit sind in den statistischen Ansatz einbezogen. Der grundlegende statistische Ansatz basiert auf der am häufigsten verwendeten Markierung für ein bestimmtes Wort in den annotierten Trainingsdaten. Diese Information wird auch zur Markierung dieses Wortes im unannotierten Text verwendet.\cite{kumawat2015pos}

\section{Named Entity Recognition}
Named Entity Recognition (NER) \cite{mansouri2008named} \cite{nadeau2007survey}\cite{partalidou2019design}\\

\section{Regelbasierte Ansätze}
Regelbasierte Ansätze: Regelbasierte Ansätze verwenden vordefinierte Regeln oder Muster, um Schlüsselwörter zu identifizieren. Dies kann beispielsweise das Extrahieren von Wörtern sein, die häufig im Text vorkommen oder bestimmten Mustern entsprechen.\\

\section{Hybride Ansätze}
In der Untersuchung mit dem Titel \citetitle{croft2000combining}\cite{croft2000combining} wird die Entwicklung von Kombinationen im Bereich des Information Retrievals analysiert. Dabei werden sowohl experimentelle Ergebnisse als auch die Retrieval-Modelle, die als formale Rahmen für die Kombination vorgeschlagen wurden, berücksichtigt. Es wird aufgezeigt, dass Kombinationsansätze für die Informationssuche als Kombination der Ergebnisse mehrerer Klassifikatoren auf der Grundlage einer oder mehrerer Darstellungen modelliert werden können. Zudem wird dargelegt, dass dieses einfache Modell Erklärungen für viele der experimentellen Ergebnisse liefern kann.\\

Die Arbeit mit dem Titel \citetitle{chiny2021lstm}\cite{chiny2021lstm} kombiniert drei Ansätze des Information Retrievals mit dem Ziel, relevante Informationen aus Produktreviews zu extrahieren. Der Ansatz \emph{TF-IDF} wird mit einem sogenannten \emph{CLASSIFIER} Model kombiniert. Das Klassifikationsmodell verarbeitet drei Eingaben der Modelle \emph{LSTM}, \emph{VADER} und \emph{TF-IDF}. Die Werte dieser Eingaben liegen im Bereich von [0,1]. Die Ausgabe des Klassifikationsmodells ist binär und gibt eine Vorhersage des vollständigen Textes der Modelleingabe aus (positiv oder negativ). Aus einem Datensatz wurden 5000 zufällige Bewertungen ausgewählt, die sich von den für die LSTM- und \emph{TF-IDF}-Modelle verwendeten Trainings- und Testdatensätzen unterscheiden. Die Autoren \citeauthor{chiny2021lstm} haben sie durch die Eingabe des globalen Modells laufen lassen, um die Vorhersagen zu erhalten, die von den Modellen \emph{LSTM}-, \emph{VADER}- und \emph{TF-IDF}-Modelle zu erhalten. Anschließend teilten sie diese Ergebnisse in zwei Stapel auf (75 \% für die Trainingsmenge und 25 \% für die Testmenge), um das binäres Klassifizierungsmodell zu trainieren und zu bewerten. Die Evaluation erfolgt durch den Einsatz der Methoden Precision, Recall und F1-Score.\\

Die Arbeit mit dem Titel \citetitle{suhasini2021hybrid}\cite{suhasini2021hybrid} befasst sich mit der Filterung von Fake news. In diesem Beitrag werden hybride Verfahren zur Gewinnung von Merkmalen untersucht, die in dem Gebiet noch nicht gründlich erforscht wurden. Die Anwendung von Hybridsystemen hat sich in einer Vielzahl von Anwendungsbereichen als nützlich erwiesen und zeigen eine Tendenz, die Fehlerquote zu reduzieren, indem sie Techniken wie \emph{TF-IDF} und \emph{N-Grams} verwenden. Es wurden Experimente unter Verwendung von Echtzeit-Twitterdaten durchgeführt. Der Datensatz umfasste ca. 5.800 Tweets, die sich auf Donald-Drummond-Geschichten bezogen. Die Sammlung und Verarbeitung der Tweets erfolgt mit Python. Der Datensatz umfasste Original-Tweets, die als gefälscht und echt gekennzeichnet wurden. Die Genauigkeit der Prognose wurde anhand der verschiedenen Nachrichten evaluiert, die für das Training verwendet wurden und am Ende mit Precision, Recall und F1-Score evaluiert.\\

Im Rahmen der Studie mit dem Titel \citetitle{darmawan2015hybrid}\cite{darmawan2015hybrid} wurde ein hybrider Algorithmus zur Extraktion von Schlüsselwörtern und Kosinusähnlichkeit zur Verbesserung der Satzkohäsion bei der Textzusammenfassung vorgeschlagen. Die vorgeschlagene Methode basiert auf einer Komprimierung von 50 \%, 30 \% und 20 \%, um Kandidaten für die Zusammenfassung zu erstellen. Die Auswertung des Ergebnisses mittels t-Test zeigt, dass die vorgeschlagene Methode den Kohäsionsgrad signifikant erhöht.\\
Der Ablauf umfasst die Analyse eines Dokuments mithilfe eines Extraktionsalgorithmus sowie die Berechnung der \emph{TF-IDF}-Werte für jeden Begriff. Anschließend werden alle \emph{TF-IDF}-Werte für jeden Satz summiert. Im nächsten Schritt werden alle Sätze anhand der Summe von \emph{TF-IDF} eingestuft. Das Kompressionsverhältnis bestimmt die Position des Satzrangs. In dieser Studie wird eine Kompression von 50 \% verwendet, was bedeutet, dass die Satzzusammenfassung um 50 \% des Originaltextes gekürzt wird. Nach der Auswahl des Satzes wird dessen Berechnung durchgeführt. Die Ähnlichkeit wird mit der Cosinus-Ähnlichkeitsmethode berechnet. Anschließend werden alle Sätze anhand ihrer Cosinus-Ähnlichkeit von der höchsten zur niedrigsten sortiert. Der resultierende Text mit neuer Satzanordnung stellt die finale Zusammenfassung dar.\\

\section{data-fusion}
data-fusion: \cite{famili1997data} \cite{frank2005comparing} \cite{bohne2013data}
\newpage

\section{Pipeline}
In der Arbeit mit dem Titel \citetitle{pirk2019implementierung}\cite{pirk2019implementierung} Arbeit wird eine Pipeline entwickelt, die die \emph{N-Gramm}-Analyse verwendet, um Schlagwörter aus einem Text zu extrahieren und mit verschiedenen Ansätzen von Word-Clouds zu visualisieren. Der Fokus dieser Studie liegt dennoch eher auf der Visualisierung als auf der Informationsgewinnung eines Textes.\\

Die Arbeit mit dem Titel \citetitle{lavin2019analyzing}\cite{lavin2019analyzing} präsentiert eine Anleitung zur Erstellung einer Pipeline mit Python und \emph{TF-IDF}. Darüber hinaus wird die Relevanz von \emph{TF-IDF} als Vorverarbeitung beim maschinellen Lernen erörtert. Im Vergleich zur rohen Termhäufigkeit weist \emph{TF-IDF} in der Regel einen höheren Vorhersagewert auf. Die Gewichtung von Themenwörtern wird erhöht, um die Bedeutung von Wörtern zu erhöhen, während die Gewichtung von hochfrequenten Funktionswörtern verringert wird. Es werden Verfahren zur Vorverarbeitung von Texten vorgestellt, die eine Umformung in die gewünschte Darstellungsform ermöglichen. Zudem werden Methoden zur Interpretation der Ergebnisse des \emph{TF-IDF}-Verfahrens erörtert. Die vorliegende Arbeit widmet sich zunächst einer detaillierten Betrachtung der zugrundeliegenden Algorithmen und ihrer Funktionsweise. Im Anschluss erfolgt die Implementierung in Python. Die Verwendung der Bibliothek \emph{sklearn} ist dabei von zentraler Bedeutung.\\

In dem Beitrag mit dem Titel \citetitle{partalidou2019design}\cite{partalidou2019design} wird ein maschineller Lernansatz für die Bereiche \emph{POS-Tagging} und \emph{NER} für die griechische Sprache unter Verwendung von \emph{spaCy} erarbeitet und evaluiert. Die Verarbeitung natürlicher Sprache wirft insbesondere bei der Analyse unüblicher Sprachen wie Griechisch Schwierigkeiten auf.\\ Der Datensatz wurde aus Texten einer griechischen Zeitung extrahiert. Die Artikel der Zeitung wurden in verschiedene Kategorien wie beispielsweise Sport, Gesundheit, Wirtschaft und politische Nachrichten eingeteilt. Die Daten bestehen aus einer Reihe von XML-Dateien, die Informationen auf der Ebene von Absätzen, Sätzen und Wörtern enthalten. Im Rahmen der Evaluation wurden verschiedene Parameter getestet, um das optimale Ergebnis zu erzielen. Der Datensatz wurde gemischt und in einen Trainingssatz, einen Testsatz und einen Validierungssatz aufgeteilt. Zur Evaluierung wurden die Methoden der Präzision, des Recalls und des F1-Scores angewendet.

\section{Ergebnis}
In der Literatur finden sich verschiedene Ansätze zur Extraktion wichtiger Stichpunkte aus einem Volltext. Eine Methode zur Ermittlung wichtiger Stichpunkte in Texten stellt die \emph{TF-IDF}-Methode (Term Frequency-Inverse Document Frequency) dar. Innerhalb einer oder mehrerer \emph{Bedarfsmeldungen} lassen sich mit dieser Methode häufig auftauchende Wörter ermitteln. Des Weiteren können graphenbasierte Methoden wie \emph{TextRank} oder \emph{YAKE} (Yet Another Keyword Extractor) zur Identifizierung von Schlüsselwörtern in Texten herangezogen werden. Ein weiterer Ansatz ist die Nutzung von \emph{N-Grammen}, die häufig vorkommende Phrasen oder Begriffe identifizieren können. Auch grammatische Kategorien von Wörtern können Rückschlüsse auf potenzielle Schlüsselwörter zulassen. Die \emph{POS-Tagging}-Methode (Part-of-Speech-Tagging) stellt eine Möglichkeit dar, um dieses Ziel zu erreichen. Des Weiteren kann \emph{NER} (Named Entity Recognition) dazu beitragen, Personen, Firmennamen, Orte, Ereignisse oder Zeitangaben zu identifizieren. Auch wenn die \emph{Bedarfsmeldungen} unstrukturiert sind, haben sich im Laufe der Zeit Konventionen entwickelt, die teilweise Strukturen eines \emph{Patterns} aufweisen können.

%spam-filter (Empfinde das Thema eventuell als zu unpassend)\\
%-Überblick über verfügbare Methoden, Herausforderungen und zukünftige Forschungsrichtungen im Bereich der Spam-Erkennung, Filterung und Eindämmung von SMS-Spam. Dabei werden auch Methodiken der keyword frequency ratio und Herunterbrechung auf keyword components behandelt \cite{shafi2017review}\\

%In diesem Beitrag werden Studien zu Technologien vorgestellt, die für die Suche und das Abrufen von Informationen im Web nützlich sind. Es wird aufgezeigt, dass Information Retrieval und Ranking im Web-Kontext anders Funktioniert als in einer statischen Datenbank. \cite{kobayashi2000information}\\

%konzeptbasiertes recruiting --> neuer Ansatz

%\section{Relevante Methoden und Techniken im Bereich Information Retrieval und Data-Mining}
%\label{sec:relevante-methoden}
\newpage





